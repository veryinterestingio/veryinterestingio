<!DOCTYPE html>
<html lang="en">
<head>
	<link rel="stylesheet" type="text/less" href="/css/post.less">
	
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="shortcut icon" type="image/png" href="/img/cat.jpg"/>
	<script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/less.js/2.5.3/less.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Roboto:400,700" rel="stylesheet">
	<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-58440568-4', 'auto');
		ga('send', 'pageview');
	</script>

	<!-- Cookie Consent plugin by Silktide - http://silktide.com/cookieconsent -->
	<script type="text/javascript">
    window.cookieconsent_options = {"message":"This website uses cookies to ensure you get the best experience on our website","dismiss":"Got it!","learnMore":"More info","link":null,"theme":"dark-bottom"};
	</script>
	<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/1.0.10/cookieconsent.min.js"></script>
	<title>ELI5: What is P value and why is 0.05 so important?</title>
</head>
<body>
	<div id="header">
	<a href="/about" title="About">About</a>
</div>
	<div id="content">
		<div class="home">
			<a href="/">Back to Home</a>
		</div>

		<ul class="posts">
<li class="post" data-handle="apa3gc">
	<div class="overview">
		<a class="source" href="https://www.reddit.com/r/explainlikeimfive/comments/apa3gc/eli5_what_is_p_value_and_why_is_005_so_important/" target="_blank" title="Reddit thread where this comes from"><i class="fa fa-external-link" aria-hidden="true"></i></a>
		<h2>
			<span class="tags tag-Mathematics">Mathematics</span>
			<a href="/posts/apa3gc" onclick="return false">What is P value and why is 0.05 so important?</a>
		</h2>
		<!--<span class="date">2019-02-14</span>-->
		<span class="is-new">NEW</span>
	</div>

		<div class="question"><span class="qa" title="Question">Q:</span><div class="markdown"><p>ELI5: What is P value and why is 0.05 so important?</p></div></div>

	<div class="comment-section">
		<div class="answers-placeholder">
			<div class="answers">
	<div class="answer" data-handle="eg6t51c">
		<a class="author" href="https://www.reddit.com/user/Xalteox" target="_blank">Xalteox</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>When doing studies that have some probability factor involved in it (say randomly sampling people), there is a chance that you may conclude a fact about something simply because of random chance in your study. Say for example it can happen that you flip a coin 4 times and every time it lands on heads. And then you conclude that all coin flips must land on heads as a result of that, which is wrong. This happened because of purely random chance.</p>
<p>A p value denotes the probability of your experiment happening the way it did the way it did. A p value of 0.05 means there is a 5% chance that the results you got support your conclusion not because there is a bias for some reason towards such results (say a coin is heaver on one side than another) but because of just dumb luck. </p>
<p>0.05 is just known as the gold standard for concluding that something is &quot;statistically significant,&quot; that is probably didn't happen because of dumb luck. But smaller p values are better, and should be possible to reach if there is a true correlation although this generally means doing a more expensive study.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="eg70vsm">
		<a class="author" href="https://www.reddit.com/user/Ufarious" target="_blank">Ufarious</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>P- Values are one of the most commonly used tools for statistical testing, but also one of the most misunderstood. Most people summarize a P-Value <em>incorrectly</em> as the &quot;likelihood that the results from your data do not show a true correlation and are just random chance.&quot; We would love to know that number (because then we could be much more confident in whether or not our data was pointing to a real connection between the test and the outcome), but we can't know that. No amount of data will tell us our error rate.</p>
<p>&#x200B;</p>
<p>A P-value <em>actually</em> tells us <strong>the likelihood that you observe this outcome in your data given that your counter-assumption is true</strong> (i.e. a true null hypothesis). Remember that in statistical testing, you have an assumption you are studying (for example, let's assume that smoking cigarettes makes you live longer) and the opposite of that assumption (smoking does not make you live longer). The assumption is called your hypothesis and the counter-assumption is called your null hypothesis. In a statistical test, we make a conclusion by <em>rejecting the null hypothesis</em>, or being able to confidently say that the opposite of our study's assumption is NOT true. I know it's confusing that statisticians take a double-negative approach, but it's the best we can do given the limits of what we can do with our math. </p>
<p>&#x200B;</p>
<p>Now, we know that smoking cigarettes doesn't help you live longer, so the counter-assumption is actually true. But there is a chance that our study will incorrectly show a connection between smoking and a longer life. If we have a P-Value of 0.05 for our data, this means that <strong>given that smoking doesn't makes us live longer, we will still see a connection between smoking and a long life 5% of the time.</strong> </p>
<p>&#x200B;</p>
<p>Most importantly, notice that the statement above tells us NOTHING about the likelihood that our assumption (or counter-assumption) is true (or false). A P-value cannot tell us how confident we are that we got the &quot;right&quot; results or how likely it is that our results were due to chance.  Given that there is no connection, random chance will will tell us there is a connection some of the time. The percent of times we can expect to get this wrong is the P-Value.  </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="eg71q7m">
		<a class="author" href="https://www.reddit.com/user/Riles_McGiles" target="_blank">Riles_McGiles</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>To give an example to help illustrate what others are saying:</p>
<p>Your friend boasts that he has an 80% free throw rate in basketball (if you're unfamiliar with basketball it's like a penalty kick in football/soccer). You are doubtful so you tell him to try 10 free throws in front of you to prove it. He makes 7 out of 10. That's less than 80%, but it's close enough that you don't want to call your friend a liar. This has a P-value of 0.32, meaning your friend, if telling the truth about hitting 80% of free throws, would 32% of the time hit 7 or fewer free throws in 10 attempts. That's pretty likely in the world of P testing.</p>
<p>However, if he only makes 1 out of 10, it's still possible for him to have been on a really unlucky streak and still make 80% overall, but the odds are so low that you can confidently call your friend a liar. This has a P-value of 0.000004, meaning if your friend is telling the truth he would miss nine out of ten 0.0004% of the time, or near impossible.</p>
<p>In this example, the 0.05 P-value cutoff is between 5 and 6 made baskets out of 10.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="eg748px">
		<a class="author" href="https://www.reddit.com/user/peroleu" target="_blank">peroleu</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>There are a lot of wrong explanations here. P value is <strong>not</strong> the probability of the results occurring by random chance. It's the probability of obtaining the results <strong>if the null hypothesis is true</strong>.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="eg7hble">
		<a class="author" href="https://www.reddit.com/user/BioSNN" target="_blank">BioSNN</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>p-values are extremely subtle and I honestly don't know if any amount of explanations would have allowed me to understand them properly when I was 5 years old. I'm not even sure I understand them now. Nonetheless, here's an attempted explanation by example.</p>
<p>Suppose you want to test the hypothesis that a coin is unfairly biased. Your null hypothesis is that it isn't biased (completely fair). Now let's say that you flip the coin 10000 times and see 5150 heads and 4850 tails. How do we tell if this is fair or not?</p>
<p>Well, if the coin was completely fair (null hypothesis), then we can calculate the probability that we'd see a deviation this large in either direction using some straightforward (but tedious) probability calculations. That is, we'll look at the sum of the probability of 5150 heads, 5151 heads, ..., as well as 4850 heads, 4849 heads, ... (for a two-sided test). We can calculate this exactly by summing over the probability mass function of the binomial distribution. In Mathematica:</p>
<p><code>1-Sum[Binomial[10000,k]0.5^k 0.5^(10000-k),{k,4851,5149}]</code></p>
<p>We'd find the probability is 0.2788% (don't worry if you don't understand how this calculation works). We can therefore say that &quot;IF the null hypothesis is true, THEN the probability of seeing a difference at least as large as 300 between heads and tails in 10000 coin flips is 0.28%&quot;. This is a p-value (p = 0.28%).</p>
<p>Okay but here's the critical part. Does this tell us whether the coin was biased (our hypothesis)? NO! (at least not directly). And there are a couple subtleties I want to point to here.</p>
<p>First: even if we were lead to believe that we could reject the null hypothesis, this does not automatically mean that our hypothesis is true. Even in this simple example, we would have to rule out hypotheses like &quot;the person who flipped the coin manipulated it to be biased towards heads (a not unreasonable hypothesis!)&quot; or &quot;the person who recorded the results made a mistake (also not unreasonable!)&quot;. In general, there are many other alternative hypotheses we would have to rule out.</p>
<p>Second: having p=0.28% doesn't even tell us whether the null hypothesis is (probably) false! We say that &quot;we reject the null hypothesis since p &lt; α = 0.05&quot;, however this is very misleading. In reality, the p-value tells us &quot;the probability [the results we saw being as extreme as they were] given [the null hypothesis is true]&quot;, rather than what we're much more interested in: &quot;the probability [the null hypothesis is true] given [the results we saw being as extreme as they were]&quot;. Notice these two statements are the same except we've flipped the clauses. It's worth reading over this a couple of times until you understand why the second is what we want, but the first is what we get with p-values.</p>
<p>Once we fully understand these subtleties, we can talk about how scientists use p-values. Essentially, they use them by ignoring the subtleties! They just say &quot;Okay, well, I think the hypothesis I'm proposing is likely to be true if the null hypothesis is false (this goes against the first subtlety) AND I think that the null hypothesis is likely to only be true if the results of this experiment fall within the range of what we'd expect if the null hypothesis is true (this goes against the second subtlety).&quot; To tell whether &quot;the results of this experiment fall within the range of what we'd expect&quot;, they set an accept/reject threshold level - α. If p &lt; α, then the thought is the probability of the null hypothesis is sufficiently low that their own hypothesis is probably true (again, this is NOT correct because of the aforementioned subtleties).</p>
<p>Now let's talk about this threshold level α. Why do we even need it? Why not report just the p-value and say that our hypothesis has a 1-p probability of being true? First, the p-value does not tell us the probability our hypothesis is true (again, see the subtleties...). Second, we often just want to classify hypotheses as either being true or false. The important thing about this threshold level is that in order for it to have any hope of being meaningful, we MUST set its value before we see the p-value. Here, we have a trade-off: set it too low, and we risk a type II error (incorrectly failing to reject a false null hypothesis); set it too high and we risk a type I error (incorrectly rejecting a true null hypothesis). The exact value of α doesn't matter, but 5% seemed reasonable. Again, the exact value doesn't matter but it nonetheless had to be (arbitrarily) chosen.</p>
<p>Recently, there's been a lot of talk about p-hacking in the news. This is unrelated to either of the subtleties I mentioned above. Using the example from above, let's suppose we ran the experiment five times for five different coins and found the following heads - tails differences for each: 130, 170, 200, 140, 90. The p-values for these different runs would be 19.7%, 9.1%, 4.66%, 16.4%, 37.3%, respectively. Notice that four of these don't pass the α = 0.05 level, but one does. If we reported on just that one, we might say we have &quot;statistically significant&quot; results that coin 3 is biased, but someone who saw our entire procedure would easily see into our deception. After all, we expect 5% or 1/20 of our runs to pass the α = 0.05 level by design, so the more we do, the more likely it is that one will pass (see <a href="https://xkcd.com/882/" target="_blank"><a href="https://xkcd.com/882/" target="_blank">https://xkcd.com/882/</a></a>). A very similar thing can happen in science. For example, you might look at the correlations between hundreds of different types of foods people eat and their longevities and conclude that certain foods cause increased longevity. But if you do this, you have to correct for the fact you're testing hundreds of hypotheses by reducing α appropriately (for example, using the Bonferroni correction).</p>
<p>&#x200B;</p>
<p>If you made it this far, you might also be interested in knowing what you'd ACTUALLY need in order to calculate the probability of the null hypothesis given the results being as extreme as they are. That is, how do we get past the second subtlety? This is where we would have to invoke Bayes' theorem: P(null hypothesis | results as extreme as they are) = P(results as extreme as they are | null hypothesis) * P(null hypothesis) / P(results as extreme as they are) = p * P(null hypothesis) / P(results as extreme as they are). Here, you should read P(A|B) as &quot;probability of A given B&quot;. So you see that while the expression we're really interested in &quot;P(null hypothesis | results as extreme as they are)&quot; is proportional to p, it is not equal to p. However, the additional factor of &quot;P(null hypothesis) / P(results as extreme as they are)&quot; is pretty much impossible to figure out objectively, which is why scientists are stuck using p-values. Basically we just hope that with a small enough value for α, the following is true: IF p &lt; α, THEN P(null hypothesis | results as extreme as they are) is small. Now you should be able to understand <a href="https://xkcd.com/1132/" target="_blank"><a href="https://xkcd.com/1132/" target="_blank">https://xkcd.com/1132/</a></a>.</p>
<p>&#x200B;</p>
<p>I hope this post was understandable, but even if it wasn't, hopefully I've instilled the proper respect for how subtle and manipulable p-values are and why scientists are becoming increasingly weary of them.</p></div>		<div class="replies-placeholder"></div>
	</div>
</div>		</div>
		<div class="more-less">
			<a class="collapse" href="javascript:void(0)">collapse</a>
			<a class="more-answers" href="javascript:void(0)">4 more answers...</a>
			<a class="less-answers lower" href="javascript:void(0)">less answers...</a>
			&nbsp;
		</div>
	</div>
	<a class="show" href="/posts/apa3gc" onclick="return false"><span>show</span></a>
</li>
		</ul>
	</div>

	<script>
		var config = {"stream":{"initial":10,"catchup":5},"api":{"url":"api.veryinteresting.io"}};
	</script>
	<script src="/js/project.js"></script>
	<script src="/js/post.js"></script>
</body>
</html>