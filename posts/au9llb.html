<!DOCTYPE html>
<html lang="en">
<head>
	<link rel="stylesheet" type="text/less" href="/css/post.less">
	
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="shortcut icon" type="image/png" href="/img/cat.jpg"/>
	<script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/less.js/2.5.3/less.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Roboto:400,700" rel="stylesheet">
	<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-58440568-4', 'auto');
		ga('send', 'pageview');
	</script>

	<!-- Cookie Consent plugin by Silktide - http://silktide.com/cookieconsent -->
	<script type="text/javascript">
    window.cookieconsent_options = {"message":"This website uses cookies to ensure you get the best experience on our website","dismiss":"Got it!","learnMore":"More info","link":null,"theme":"dark-bottom"};
	</script>
	<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/1.0.10/cookieconsent.min.js"></script>
	<title>8 bit, to 16 bit, to 32 bit, to 64 bit. Much of the focus in the computing industry seems to be &quot;what's going to replace silicon&quot;, but what about something like 128-bit architecture? Is such a thing even possible?</title>
</head>
<body>
	<div id="header">
	<a href="/about" title="About">About</a>
</div>
	<div id="content">
		<div class="home">
			<a href="/">Back to Home</a>
		</div>

		<ul class="posts">
<li class="post" data-handle="au9llb">
	<div class="overview">
		<a class="source" href="https://www.reddit.com/r/askscience/comments/au9llb/8_bit_to_16_bit_to_32_bit_to_64_bit_much_of_the/" target="_blank" title="Reddit thread where this comes from"><i class="fa fa-external-link" aria-hidden="true"></i></a>
		<h2>
			<span class="tags tag-Computing">Computing</span>
			<a href="/posts/au9llb" onclick="return false">8 bit, to 16 bit, to 32 bit, to 64 bit. Much of the focus in the computing industry seems to be &quot;what's going to replace silicon&quot;, but what about something like 128-bit architecture? Is such a thing even possible?</a>
		</h2>
		<!--<span class="date">2019-02-27</span>-->
		<span class="is-new">NEW</span>
	</div>

		<div class="question"><span class="qa" title="Question">Q:</span><div class="markdown"><p>Just a random question I thought of when looking into performance jumps between the different bit architectures. If there has been any research into the topic, have any of those investigations turned up any theories on possible increases in processing power?</p></div></div>

	<div class="comment-section">
		<div class="answers-placeholder">
			<div class="answers">
	<div class="answer" data-handle="eh6r5o9">
		<a class="author" href="https://www.reddit.com/user/turbo_speedwagon" target="_blank">turbo_speedwagon</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><blockquote>
<p>what about something like 128-bit architecture? Is such a thing even possible?</p>
</blockquote>
<p>As much as 1990s video game console designers wished that register width / bits were the measure of performance, they aren't.  It's possible but won't give any benefit, so nobody is interested in it.</p>
<p>Adding more bits to a register means more data to ship in and out of memory and disk, which takes time.  By itself, it can make a system slower.  So engineers only increase the register width when there are other motivating factors, otherwise keep things smaller and simpler by default as a general engineering principle.</p>
<p>One of the things that has pushed us to 16, 32, then 64 bits is memory.  You can only represent numbers up to \~64K (65536, 2 to the 16th power) with 16 bits.  This means the processor cannot address more than 64 kilobytes of RAM - that's not much by today's standards, is it?  So they increased the bits to 32 bits, to handle more RAM.  Many years later, people wanted more than 4 gigabytes of ram (2 to the 32nd power, 32 bits), so they increased to 64 bits.  Now we're at 64 bits, which can go up to 16,000,000 TERABYTES which should last a while. So no need to increase it here.</p>
<p>Another motivating factor is integer size, which is limited by bits the same way as memory.  If I write a program that processes all 7 billion people in the world, you can't assign every person a number in 16 bit (only 64K possible values) or even 32 bit (4 billion possible values).  So as data sizes get bigger, we need to be able to handle bigger numbers.  This is not an issue for 64 bit numbers which go all the way up to about 18,000,000,000,000,000,000.  For numbers bigger than that, which we can handle, we can stick with floating point, which has less precision; there's no current mainstream need to go up to 128 and have bigger numbers.  We can also treat integers bigger than that with 128, 256, or any number of bits on our current processors, it's just slower.</p>
<p>Technically, inside most processors now are 128 bit registers.  But they don't do math on 128 bit numbers.  They are treated as 4 separate, 32 bit numbers (or 2 64 bit numbers, etc) .  This way the processor can do 4 operations on 4 numbers with 1 instruction, this is called SIMD.  Why not increase this more?  They did, they upped it to 256 bit, and now we're at 512 bit, can operate on 16 numbers with a single instruction.  You could call it &quot;512 bit&quot; in a sense, but most purists would still call it 64 bit.  The memory bus and core registers are still 64 bit, this is just like its own little thing on the side.</p>
<p><strong>Edit:</strong> I see replies about using software to do math on more bits. I strongly agree.  The number of bits on a processor does not limit the size of numbers that can be processed, only the size that can be processed most quickly and efficiently.  Bigger numbers (more bits) require more operations and are therefore slower, so adding more bits to the CPU makes the processor faster by reducing to one operation again.  To OP's original question, this only makes it faster when most operations require more bits than available (not the case here in 64 bit CPUs).</p>
<p>Just to clarify on a similar note, the bits on a processor don't necessarily limit memory usage either.  People have used something called &quot;bank switching&quot; to go over 64K of memory on 16 bit processors, this was more commonplace in the 1980s era processors.  This may or may not make the system slower depending on the implementation and algorithm.</p>
<p>When bank switching and multiple operations for math calculations become too commonplace, reducing performance too much, chip designers can increase the number of bits to fix both problems and get a big performance boost.  Bank switching and multi-operation math are not major hindrances with our current 64 bit processors, so increasing to 128 bit won't speed it up where it could before.  </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="eh6n3ca">
		<a class="author" href="https://www.reddit.com/user/bobert680" target="_blank">bobert680</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>That refers to the size of the memory bus on the CPU and determines the number of addressable locations there are in memory. so a 64 bit CPU can talk to 2^64 (18,446,744,073,709,551,616 bits, 2 zettabyte) of ram. Meaning we are not even close to needing to go with 128 bit architecture yet and will get actuall improvements in performance from other areas.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="eh6umpe">
		<a class="author" href="https://www.reddit.com/user/homura1650" target="_blank">homura1650</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>There are a lot of downsides to increasing the datawidth of computers, and few upsides.</p>
<p>Much of what I am saying is generally applicable, but for concreteness, I will talk specifically about the x86 and x84_64 architectures in places; which are the predominant 32 and 64 bit architectures in non-smartphone consumer computers.</p>
<p>The main performance bottleneck for most programs is actually the size of code and data in memory. In particular, while modern computers almost always have memory measured in the gigabytes these days, that memory is actually very far away from the CPU and very slow to access. To speed up execution, CPUs will store a copy of recently accessed memory in a cache so that they do not need to go back to main memory. Even within a CPU, making caches bigger slows them down for the same reason that main memory is slow; so modern CPUs will typically have three layers of cache. To pick a random example, the Ryzen 5 2600 has:</p>
<ul>
<li>L1 cache: 64kb instruction and 32kb data (per core)</li>
<li>L2 cache: 512kb (per core)</li>
<li>L3 cache: 16mb (split into 2 caches; not entirely sure how they are divided)</li>
</ul>
<p>Each layer of cache is slower than the last (and RAM is slower than any cache), so it is critical for performance to stay inside of caches as much as possible. Doubling the size of your pointers greatly reduces how much you can fit inside of a cache.</p>
<p>The main reason to increase the size of your pointers is that the maximum size of you RAM is limited by the datawidth. For instance, in a 32 bit processor, RAM is addressed by 32 bits integers, which means there are only 2^32 addresses available. This limits you to 4GB. If this is enough for you, then there is no downside to using 32 bits. However, if it is not enough for you, you have to start going to harddrives, which are even slower than ram. [0].</p>
<p>However, in practice there is another reason that increasing the data width may improve performance. Gennerally speaking CPU designers take backwards comparability very seriously. When Intel releases the 9700, they do not want anyone to have to not buy it because they are concerned it won't be able to run software that worked on the 8700. In fact, even the newest x86_64 processors are still able to run 16 bit programs written for the original 8086 [1].</p>
<p>However, increasing the datawidth is not backwards compatible. Processors work around this by offering a mode where still run programs as if they were using a smaller datawidth. However, to be able to take advantage of the larger datawidth, a program must be recompiled to target it. Since a recompilation is nessasary anyway, the designers of x86_64 took the oppurunity to make other, unrelated, improvements to the architecture.</p>
<p>To add some perspective to this, the Linux kernel offers what is known as x32. Programs running under x32 are restricted to using 32 bit pointers, but can take advantage of all the other improvements that x86_64 made.</p>
<p>In principle there is another theoretical advantage to going beyond 64 bits, even if you never come close to the 16 exabyte limit. As I mentioned earlier, modern computers use virtual memory, which allows multiple processes on the same machine to use the same logical address to refer to a different area of physical memory. There are advantages and disadvantages to this approach. One of the disadvantages is that it means addresses are not comparable between processes. Eg. it is is generally not meaningful to pass pointers between processes. However there is another extreme which is globally unique addresses. The idea here is not that within a single computer, all programs should use the same address space, but across the entire planet (or datacenter), all programs should use the same address space. This is exciting because it means that, in theory, you can have a pointer to a piece of data and pass that pointer around to different machines. Then, when any machine wants to read the data they can &quot;just&quot; look on the network for which machine has it and get it from them. One of the problems to be solved here is routing. If you want the data located as 0x12345678, you likely do not have a map saying exactly where it is. So one approach is to divide the address space into geographical regions. So you could send your request to the datacenter that owns 0x12XXXXXX, it can then forward it to the rack that owns 0x1234XXXX, who can forward it to the box that owns 0x123456XX, who can find the result in memory. The issue here is that, at any level, you might want to increase the amount of memory. This means that you need to overprovision the amount of address space that each machine/rack/datacenter has to avoid any risk of running out.</p>
<p>Currently the above is still theoretical, but to get some perspective on the amount of &quot;wasted&quot; address space this would need, we can look at IPv6, which needs to solve an almost identical problem of assigning IP addresses (arguably, it is an exactly identical problem). When you request an IPv6 address it is common to be given an entire /64. That is to say, you are not given a full 128 bit address (which is the size of IPv6 addresses), but rather the first 64 bits of an address. This gives you every address that starts with those 64 bits, which means you are actually being given 2^64 addresses. This is enough to give every byte of address space on a 64 bit system its own IP address. </p>
<p>There are also some reasons to introduce memory spareness within a process. For instance, a common source of vulnerabilities is buffer overflows, where you try accessing an array past its end. In principle, if you have a 512 byte buffer, you could store it in its own 1kb logical address space (backed by only 512 bytes of physical memory), then allow the CPU page table to act as an automatic bounds checker. Currently, the architecture of modern CPUs do not allow extensive use of this technique, but it is another potential reason we might at some point need a bigger address space.</p>
<p>[0] There are some tricks where a 32 bit processor can use more than 4GB. Essentially (for unrelated reasons) CPUs already differentiate between physical and logical address. So, on an x86, each process is given the full 4GB of logical addresses, regardless of the amount of RAM that is available. The CPU than has a &quot;page table&quot; which maps these logical addresses to physical addresses. Even on the 32 bit x86, there is a feature calles Physical Address Extension (PAE) that allows for having 64 bit physical addresses with only 32 bit logical addresses. This means that, as long as each individual process needs only 4GB of memory, your entire system can take advantage of the full 16 exabytes of RAM that you could theoretically give it. </p>
<p>It is also worth pointing out, that simply being 64 bits, does not mean that a CPU operates only on units that are 64 bits big. For instance, x86_64 has 16 general purpose 64 bit registers [2]. It also has 8 80-bit floating point registers, and 16 128-bit SSE registers.</p>
<p>[1] At least when the processor is in real-mode. One of the first thing modern operating systems do is disable real-mode because they want to be able to use features not present in the early architectures (include, but not limited to, more than 16 bits worth of memory).</p>
<p>[2] You can still access the lower 8, 16, or 32 bits of them as their own register, but I will ignore this for the moment.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="eh6s0gs">
		<a class="author" href="https://www.reddit.com/user/theartdeco" target="_blank">theartdeco</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>16-bit processor doesn’t do operations faster than 8-bit but it can do the same operations (multiplication, division etc)  on larger values. For example, a 16-bit processor can usually add 10000 to 5000 in one cycle but 8-bit processor doesn’t really understand values larger than 256, so if you want to add 10000 and 5000 you have to store both values as combinations of two 8-bit values, so you end up with 4 values and then you need to do a complicated set of operations on those 4 values just to add 10000 to 5000. 8-bit processor will take way more than 1 cycle to do all those operations.
Now, to answer you question: 64-bit values can be very very large - as large as 18,446,744,073,709,551,615. Most of the time the data values that we want to work on fit perfectly well in 64 bits, so there is really no need for 128-bit processor. </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="eh6socu">
		<a class="author" href="https://www.reddit.com/user/riley_sc" target="_blank">riley_sc</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>In terms of having enough bits to work with, 64 gives you an insane range for integers and more accuracy with decimal numbers than scientific experiments can reach, so there’s no significant need for more bits in general purpose computing.  </p>
<p>Modern CPUs actually do have 128 and 256 bit registers, though their main use isn’t to represent big numbers but to perform math on multiple values at a time. This is called SIMD for single instruction, multiple data. </p></div>		<div class="replies-placeholder"></div>
	</div>
</div>		</div>
		<div class="more-less">
			<a class="collapse" href="javascript:void(0)">collapse</a>
			<a class="more-answers" href="javascript:void(0)">4 more answers...</a>
			<a class="less-answers lower" href="javascript:void(0)">less answers...</a>
			&nbsp;
		</div>
	</div>
	<a class="show" href="/posts/au9llb" onclick="return false"><span>show</span></a>
</li>
		</ul>
	</div>

	<script>
		var config = {"stream":{"initial":10,"catchup":5},"api":{"url":"api.veryinteresting.io"}};
	</script>
	<script src="/js/project.js"></script>
	<script src="/js/post.js"></script>
</body>
</html>