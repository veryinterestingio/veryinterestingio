<li class="post" data-handle="oz3x50">
	<div class="overview">
		<a class="source" href="https://www.reddit.com/r/askscience/comments/oz3x50/what_is_p_hacking/" target="_blank" title="Reddit thread where this comes from"><i class="fa fa-external-link" aria-hidden="true"></i></a>
		<h2>
			<span class="tags tag-Mathematics">Mathematics</span>
			<a href="/posts/oz3x50" onclick="return false">What is P- hacking?</a>
		</h2>
		<!--<span class="date">2021-08-09</span>-->
		<span class="is-new">NEW</span>
	</div>

		<div class="question"><span class="qa" title="Question">Q:</span><div class="markdown"><p>Just watched a ted-Ed video on what a p value is and p-hacking and Iâ€™m confused. What exactly is the P vaule proving? Does a P vaule under 0.05 mean the hypothesis is true? </p>
<p>Link: <a href="https://youtu.be/i60wwZDA1CI" target="_blank">https://youtu.be/i60wwZDA1CI</a></p></div></div>

	<div class="comment-section">
		<div class="answers-placeholder">
			<div class="answers">
	<div class="answer" data-handle="h7xts6m">
		<a class="author" href="https://www.reddit.com/user/wattnurt" target="_blank">wattnurt</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>All good explanations so far, but what hasn't been mentioned is WHY do people do p-hacking.</p>
<p>Science is &quot;publish or perish&quot;, i.e. you have to submit scientific papers to stay in academia. And because virtually no journals publish negative results, there is an enormous pressure on scientists to produce a positive results.</p>
<p>Even without any malicious intent by the scientist, they are usually sitting on a pile of data (which was very costly to acquire through experiments) and hope to find something worth publishing in that data. So, instead of following the scientific ideal of &quot;pose hypothesis,  conduct experiment, see if hypothesis is true. If not, go to step 1&quot;, due to the inability of easily doing new experiments, they will instead consider different hypotheses and see if those might be true. When you get into that game, there's a chance you will find. just by chance, a finding that satisifies the p &lt; 0.05 requirement.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="h7y8402">
		<a class="author" href="https://www.reddit.com/user/inborn_line" target="_blank">inborn_line</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Here's an example that I've seen in the real world. If you're old enough you remember the blotter paper advertisements for diapers. The ads were based on a test that when as such:</p>
<p>Get 10 diapers of type a &amp; 10 diapers of type b. </p>
<ol>
<li>Dump w milliliters of water in each diaper.</li>
<li>Wait x minutes</li>
<li>Dump y milliliters of water in each diaper</li>
<li>Wait z minutes</li>
<li>Press blotter paper on each diaper with q force.</li>
<li>Weigh blotter paper to determine if there is a statistical difference between diaper type a and type b</li>
</ol>
<p>Now W &amp; Y should be based on the average amount of urine produced by an infant in a single event. X should be based on the average time between events. Z should be a small amount of time post urination to at least allow for the diaper to absorb the second event.  And Q should be an average force produced by an infant sitting on the diaper. </p>
<p>The competitor of the company I worked for did this test and claimed to have shown a statistically significant difference with their product out-performing ours. We didn't believe this to be true so we challenged them and asked for their procedure. When we received their procedure we could not duplicate their results. Additionally, if you looked at their process, it didn't really make sense. W &amp; Y were different amounts, X was too specific an amount of time (in that, for this type of test it really makes the most sense to use either a specific time from the medical literature or a round number close to that  (so if the medical literature pegs the average time between urination as 97.2 minutes, you are either going to test 97.2 minutes or 100 minutes, you are not going to test 93.4 minutes). And Q suffered from the  same issue as X. </p>
<p>As soon as I saw the procedure and noted our inability to reproduce their results, I knew that they had instructed their lab to run the procedure at various combinations of W,X,Y,Z, and Q. If they didn't get the result they wanted, throw out the results and choose a new combination. If they got the results they wanted stop testing and claim victory. While the didn't admit that this was what they'd done, they did have to admit that they couldn't replicate their results either. Because the challenge was in the Netherlands, our competitor had to take out newspaper ads admitting their falsehood to the public.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="h7yxigk">
		<a class="author" href="https://www.reddit.com/user/Fala1" target="_blank">Fala1</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Good chance this will just get buried, but I'm not all that satisfied with most answers here.</p>
<p>So the way most science works is through null-hypotheses. A null-hypothesis is basically an assumption that there is no relationship between two things.   </p>
<p>So a random example: a relationship between taking [vitamin C] and [obesity].<br />
The null-hypothesis says: There is no relationship between vitamin C and obesity.<br />
This is contrasted with the alternative-hypothesis. The alternative-hypothesis says: there is a relationship between the two variables. </p>
<p>The way scientists then work is that they conduct experiments, and gather data. Then they interpret the data.<br />
And then they have to answer the question: Does this support the null-hypothesis, or the alternative-hypothesis?<br />
The way that works is that the null-hypothesis is assumed by default, and the data has to prove the alternative-hypothesis by 'disproving' the null-hypothesis, or else there's no result.</p>
<p>What researchers do is before they conduct the experiment is they set an alpha-value (this is what the p-value will be compared against).<br />
This has to be set because there's two types of errors in science: You can have false-positives, and false-negatives.<br />
The alpha-value is directly related to the amount of false positives. If it's 5% then there's a 5% chance of getting a false positive result. It's also indirectly related to false-negatives though. Basically, the stricter you become (lower alpha value), the less false-positives you'll get. But at the same time, you can also become so strict that you're throwing away results that were actually true, which you don't want to do either.<br />
So you have to make a decision to balance between the chance of a false-positive, and the chance of a false-negative.<br />
The value is usually 5% or 0.05, but in some fields of physics it can be lower than 0.0001</p>
<p>This is where p-values come in.<br />
P-values are a result of analyzing your data, and what it measures is kind of the randomness of your data.<br />
In nature, there's always random variation, and it's possible that your data is just the result of random variance.<br />
So we can find that Vitamin C consumption leads to less obesity, and that could either be because 1) vitamin C does actually affect obesity, but it could also just be that 2) the data we gathered happened to show this result by pure chance, and that there is actually is no relationship between the two: It's just a fluke.  </p>
<p>If the p-value you find is lower than your alpha-value. Say it's 0.029 (which is smaller than 0.05), you can say &quot;The chance that we found these result by pure chance (meaning no relationship between the variables) is less than 5%, but this is a very small chance, so we can actually assume that there actually is a relationship between the variables&quot;.<br />
This p-value then leads to the rejection of the null-hypothesis, or in other words: we stop assuming there is no relationship between the variables. We may start assuming there is a relationship between the variables. </p>
<p>The issue where p-hacking comes in is that the opposite isn't true.<br />
If we fail to reject the null-hypothesis (because the p-value wasn't small enough) you <strong><em>do not</em></strong> accept the null-hypothesis as true.<br />
Instead, you may only conclude that the results are inconclusive.<br />
And well, that's not very useful really. So if you want to publish your experiment in a journal, drawing the conclusion &quot;we do not have any conclusive results&quot; is well.. not very interesting. And that's why historically, these papers either aren't submitted, or are rejected for being published.   </p>
<p>The reason why <em>that</em> is a major issue is because by design, when using an alpha-value of 5%, 5% of the studies will be due to random variance and not due to an actual relationship between variables.<br />
So if 20 people do the same study, one of them will find a positive result, and 19 of them won't.<br />
If those 19 studies then get rejected for publishing, but the one studies does get published, then people reading the journals walk away with the wrong conclusion.<br />
This is known as the &quot;file-drawer problem&quot;. </p>
<p>Alternatively, there are researcher that basically commit fraud (either light fraud, or deliberate cheating). Because their funding can be dependent on publishing in journals, they have to come out with statistically significant results (rejecting of the null-hypothesis). And there's various ways they can make small adjustments to their studies that increases the chance of finding a positive result, so they can get published and receive their funding.<br />
You can run multiple experiments, and just reject the ones that didn't find anything. You can mess with variables, make multiple measurements, mess with sample sizes, or outright change data, and probably more.  </p>
<p>There are obvious solutions to these problems, and some of them are being discussed and implemented. Like agreeing to publish studies before knowing their results. Better peer-review. More reproducing of other studies, etc.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="h7xf80i">
		<a class="author" href="https://www.reddit.com/user/Astrokiwi" target="_blank">Astrokiwi</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Suppose you have a bag of regular 6-sided dice. You have been told that some of them are weighted dice that will always roll a 6. You choose a random die from the bag. How can you tell if it's a weighted die or not?</p>
<p>Obviously, you should try rolling it first. You roll a 6. This <em>could</em> mean that the die is weighted, but a regular die will roll a 6 sometimes anyway - 1/6th of the time, i.e. with a probability of about 0.17.</p>
<p>This 0.17 is the p-value. It is the probability that your result isn't caused by your hypothesis (here, that the die is weighted), and is just caused by random chance. At p=0.17, it's still <em>more likely than not</em> than the die is weighted if you roll a six, but it's not very conclusive at this point(Edit: this isn't actually quite true, as it actually depends on the fraction of weighted dice in the bag). If you assumed that rolling a six meant the die was weighted, then if you actually rolled a non-weighted die you would be wrong 17% of the time. Really, you want to get that percentage as low as possible. If you can get it below 0.05 (i.e. a 5% chance), or even better, below 0.01 or 0.001 etc, then it becomes extremely unlikely that the result was from pure chance. p=0.05 is often considered the <em>bare minimum</em> for a result to be publishable.</p>
<p>So if you roll the die twice and get two sixes, that still could have happened with an unweighted die, but should only happen 1/36~3% of the time, so it's a p value of about 0.03 - it's a bit more conclusive, but misidentifying an unweighted die 3% of the time is still not amazing. With 3 dice you get p~0.005, with 4 dice you get p~0.001 and so on. As you improve your statistics with more measurements, your certainty increases, until it becomes extremely unlikely that the die is not weighted.</p>
<p>In real experiments, you similarly can calculate the probability that some correlation or other result was just a coincidence, produced by random chance. Repeating or refining the experiment can reduce this p value, and increase your confidence in your result.</p>
<p><em>However</em>, note that the experiment above only used one die. When we start rolling multiple dice at once, we get into the dangers of p-hacking.</p>
<p>Suppose I have 10,000 dice. I roll them all once, and throw away any that don't have a 6. I repeat this three more times, until I am only left with dice that have rolled four sixes in a row. As the p-value for rolling four sixes in a row is p~0.001 (i.e. 0.1% odds), then it is extremely likely that all of those remaining dice are weighted, right?</p>
<p>Wrong! This is <em>p-hacking</em>. When you are doing multiple experiments, the odds of a false result increase, because every single experiment has its own possibility of a false result. Here, you would expect that approximately 10,000/6^(4)=8 unweighted dice should show four sixes in a row, just from random chance. In this case, you shouldn't calculate the odds of each individual die producing four sixes in a row - you should calculate the odds of any out of 10,000 dice producing four sixes in a row, which is much more likely.</p>
<p>This can happen intentionally or by accident in real experiments. There is a <a href="https://xkcd.com/882/" target="_blank">good xkcd</a> that illustrates this. You could perform some test or experiment on some large group, and find no result at p=0.05. But if you split that large group into 100 smaller groups, and perform a test on each sub-group, it is likely that about 5% will produce a false positive, just because you're taking the risk more times. For instance, you may find that when you look at the US as a whole, there is no correlation between, say, cheese consumption and wine consumption at a p=0.05 level, but when you look at individual counties, you find that this correlation exists in 5% of counties. Another example is if there are <em>lots</em> of variables in a data set. If you have 20 variables, there are potentially 20*19/2=190 potential correlations between them, and so the odds of a random correlation between some combination of variables becomes quite significant, if your p value isn't low enough.</p>
<p>The solution is just to have a tighter constraint, and require a lower p value. If you're doing 100 tests, then you need a p value that's about 100 times lower, if you want your individual test results to be conclusive.</p>
<p>Edit: This is also the type of thing that feels really opaque until it suddenly <em>clicks</em> and becomes obvious in retrospect. I recommend looking up as many different articles &amp; videos as you can until one of them suddenly gives that &quot;aha!&quot; moment.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="h7ym1j2">
		<a class="author" href="https://www.reddit.com/user/BadFengShui" target="_blank">BadFengShui</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>I have a &quot;fun&quot; real-world example I ran into years ago. A study purported to have found a correlation between vaccines and autism, so I made sure to <em>actually read the research</em>.</p>
<p>The study found a link between a particular vaccine and autism rates in black boys, aged 1.5-3yo (or thereabouts; I don't recall the exact age range). Assuming that vaccines don't cause autism, the probability, p, of getting so many autistic children in that sample was less than 5%. More plainly: it's really unlikely to get that result if there is no correlation, which <em>seems</em> to suggest that there is a correlation.</p>
<p><em>Except</em> it wasn't a study on black boys aged 1.5-3yo: it was a study on all children. No link was found for older black boys; no link was found for non-black boys; no link was found for any girls. By sub-dividing the groups over and over, they effectively changed their one large experiment into <em>dozens</em> of smaller experiments, which makes finding a 1-in-20 chance a lot more likely.</p></div>		<div class="replies-placeholder"></div>
	</div>
</div>		</div>
		<div class="more-less">
			<a class="collapse" href="javascript:void(0)">collapse</a>
			<a class="more-answers" href="javascript:void(0)">4 more answers...</a>
			<a class="less-answers lower" href="javascript:void(0)">less answers...</a>
			&nbsp;
		</div>
	</div>
	<a class="show" href="/posts/oz3x50" onclick="return false"><span>show</span></a>
</li>
