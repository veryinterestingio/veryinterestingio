	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/UniversityofBath" target="_blank">UniversityofBath</a>
			<div class="markdown"><p>Hi /u/StringOfLights thank you for the question! Happy to hear you're interested in <a href="https://twitter.com/mammalweb" target="_blank">MammalWeb</a>, please do follow us <a href="https://twitter.com/mammalweb" target="_blank">on Twitter</a> if you use it.</p>
<blockquote>
<p>What sort of limitations do you have on such a dataset, and how do you control for differences between participants?</p>
</blockquote>
<p>Great points! Haha one of these days I hope we can sit down and chat about this over a beer (or your beverage of choice). :)</p>
<p>This is a hugely challenging part of crowdsourcing, which is <em>one part</em> of citizen science, that academic scientists are studying. I can go on for a long time about this, but I'll use one example that we've published <a href="https://zslpublications.onlinelibrary.wiley.com/doi/full/10.1002/rse2.84" target="_blank">in this open access paper</a>. When people visit our website to classify wildlife images, there will always be people who mis-identify animals or do some other thing to throw things off (possibly as a result of how the user interface is designed). To study this and how to account for these variations, we learned from <a href="https://conbio.onlinelibrary.wiley.com/doi/10.1111/cobi.12695" target="_blank">past work</a> and tried to model how people's inputs relate to what the &quot;correct answer/ID&quot; is for the images. Through statistical modelling, we treated user-supplied classifications as &quot;votes&quot; for whether a species is present in a image and applied that to a large dataset of, say, 10,000s of photos. The model lets us estimate the number of votes &quot;for&quot; a species is needed for a confident classification.</p>
<p>To give a wild example, the model (using existing data) might say that to be 95% sure that an elephant is in a photo, you only need positive identification from two people. OTOH for harder-to-identify species such as a small mouse (and many rodents look similar!), you might only be 60% sure even if 10 people have said its there. Having measures like these guides us to manually check photos where the certainty is low and saves lots of time.</p>
<blockquote>
<p>Do you see variability in camera trap types or settings, and how does that factor in?</p>
</blockquote>
<p>There is some variability yes. We ask MammalWeb citizen scientists to follow a sampling field protocol where important metadata like the make/model of the camer trap is recorded (people bring different kinds of cameras into the field or just their own backyards), and the idea is to account for these things in our modelling. Of course other crucial metadata like location, deployment duration, and other other things are recorded as well.</p>
<p>In my experience, camera traps do vary in terms of their performance and you get what you paid for. There is a reason why you pay US$500+ for a camera trap that is sensitive and has quick reaction time to animal movement (which triggers image capture) and is weather proof!</p></div>		</li>
					</ul>
	