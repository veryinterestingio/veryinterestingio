	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/acwgough" target="_blank">acwgough</a>
			<div class="markdown"><p>Alex: </p>
<p>Hi, this is a great question! There are certainly things that would be hugely benefited with 10x or 100x computational power in cosmology, for a couple of different reasons. Up until very recently, cosmology has been a science without an overwhelming amount of data; for example it wasn’t too long ago that the Hubble parameter (one of the most basic numbers in cosmology) wasn’t known to better than a factor of two. We now know the Hubble parameter to “percent level accuracy” meaning the error on the value is measured in single percent numbers, e.g. less than \~10% (though there are some other problems with the Hubble parameter in particular, see some other questions about that). Because of this, a lot of high precision stuff is done via numerical simulations, which start with some initial conditions and then evolve those conditions forwards according to the models of physics we have for the things involved. However, numerical simulations are computationally expensive and fairly slow (if you want big volumes and good resolution), and come with their own hosts of problems. I am not really involved with the numerical/computational side of things, so I can’t comment on it more precisely than that, perhaps one of my colleagues can though.</p>
<p>However, we’ve recently entered and are about to really get into the era of “<strong>large data cosmology</strong>” where we’ll have huge amounts of data, and can really get into constraining cosmological models really tightly. This huge amount of data comes with a downside though, which is that there is just <em>so much of it</em>. The Legacy Survey of Space and Time (LSST) for example will generate about 20 terabytes of data each night for 10 years, resulting in a 20 petabyte catalogue. With 10x or 100x better computation, I imagine we could extract much more information from these sorts of catalogues than we could otherwise, and make use of much more of the data in general.</p>
<p>That said, my work, and the work of many other theorists in general, can work in harmony with the numerics side of things. For example, the work theorists do can help efficiently identify what things are worth simulating (because they tell us about things we want to know about) and what things we can afford to ignore. Theory can also sometimes find analytic expressions or approximations for quantities or processes that were previously brute forced numerically (because there wasn’t a better way), which can also hugely speed up computation. My supervisor and collaborators has recently applied one of these kinds of theoretical techniques to how we can better set up the initial conditions in simulations, which if you’re interested you can see here: <a href="https://arxiv.org/abs/2008.09124" target="_blank"><a href="https://arxiv.org/abs/2008.09124" target="_blank">https://arxiv.org/abs/2008.09124</a></a></p>
<p>TL;DR: theoretical work and approximations can help us focus on what is worth computing and what the best ways to do that are, but everything could benefit from having more computational power, as we could run finer resolution or bigger simulations and could extract more information from the upcoming survey experiments.</p></div>		</li>
					</ul>
	