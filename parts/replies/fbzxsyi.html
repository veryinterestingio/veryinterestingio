	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/alanharker" target="_blank">alanharker</a>
			<div class="markdown"><p>I like this explanation.
I think the other thing which people struggle to fathom because we sort of lack a daily frame of reference are the sizes and numbers involved- a hair is about 80,000nm wide, and if you could somehow break off individual transistors from a 9900K you'd need less than 2.5 CPUs (two and a half, not a typo) to give one transistor to every single human on Earth.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/shadoor" target="_blank">shadoor</a>
			<div class="markdown"><p>I think you were planning to go somewhere with the hair width but never made it.</p>
<p>:O</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/alanharker" target="_blank">alanharker</a>
			<div class="markdown"><p>Oh like, when youre talking about a &quot;7nm process node&quot;, just how small 7nm is in real terms. </p>
<p>I mean thats like 3.5 DNA strands wide but I feel like thats just as useless a metric.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ImprovedPersonality" target="_blank">ImprovedPersonality</a>
			<div class="markdown"><p>I’ve already replied this to another answer: Why is it this incremental? Why is there not a revolutionary discovery of a new material or process which allows you to shrink to 10% the size? Why is EUV only allowing them to go from 14nm to 7nm and not to 1nm? Why do they manage to squeeze out a few more instructions per clock cycle every year and not have a 200% increase in the first few generations (as the design matures) and then hit a wall? At some point there has to be a limit in what you can do to a prefetch unit, instruction decoder and branch predictor.</p>
<p>The strange thing is that hard disks also follow a similar progression. Helium and Shingled Magnetic Recording only allowed them to continue the trend of doubling capacity every few years instead of making a true revolution in capacity possible.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/randxalthor" target="_blank">randxalthor</a>
			<div class="markdown"><p>Three parts to this:<br />
1) Revolutionary advances are risky.<br />
2) Revolutionary advances take a long time to mature.<br />
3) Revolutionary advances are generally made at a component level, rather than the entire system.  </p>
<p>Take SSDs as a perfect example:  </p>
<p><strong>Risk:</strong> When they were first introduced to the market, a lot of R&amp;D had already gone into producing what was a mostly inferior product to magnetic hard drives. But, the manufacturers bet on a user demographic that would want the very specific revolutionary performance improvements seen in SSDs: very fast access to files. It was fantastic for certain servers that needed speed at any cost. Optane was a similar bet which is much earlier in its life cycle.  </p>
<p><strong>Maturity:</strong> SSDs had to compete with hard drives across the entire storage market. With a relatively immature technology, the drives had great performance in random read applications and low power consumption. However, the drives degraded quickly (lower MTBF than HDDs) and cost orders of magnitude more. After years of further development, they cost hundreds of times less per GB and had far higher longevity than when they were first released.  When a new technology comes into play, its theoretical performance could be orders of magnitude higher than existing tech. However, it takes time to squeeze that out in a practical setting. So, you see a decade in advance that &quot;new tech Y is 10x better than old tech X.&quot; However, maybe it takes 10 years to develop it. In that time, incremental improvements to product X made it 8x its original performance. Then, when product Y finally reached the market, it's only 25% better than product X. This happens all the time. Optane was touted as 10x faster than traditional NAND SSDs, but SSDs got a lot faster before Optane actually hit the market. Intel and AMD have plans for chips 10 years from now that will be X% faster than current tech, but they also have plans for 8 years from now, and 6, and 4 and 2. All of those are are different points in their decade+ development cycles.</p>
<p><strong>Component vs. System:</strong> The revolutionary improvement in SSDs was the non volatile flash memory. However, if you flip over a hard drive, you'll see a circuit board with a number of components on it, rather than just a box with spinning disks (the actual storage media). There's a cache, there's a controller, there are motors that control the read/write heads.  </p>
<p>If you triple the speed of your read/write heads, you might get a 10% improvement on your seek times. Why? Because your HDD platters are still spinning at the same speed.  </p>
<p>Likewise, SSDs introduced memory that was many times as fast as spinning a platter with physical heads. However, the controllers had limited throughput, capping the maximum sequential read and write performance. The SATA interconnect cables were designed for HDDs and thus weren't fast enough to take advantage of the speed available from flash chips. Wear leveling algorithms hadn't been developed far, and write operations were more damaging, so SSDs didn't last through many write cycles. Transistor density was much lower, so a chip held far less flash than it does now. Massively improving the performance of one part of a system usually only moves the bottleneck somewhere else and you end up with a small overall performance improvement.  </p>
<p>So, you get incremental improvements because it's risky and expensive if a big leap fails to meet expectations (see Optane and Intel's 10nm process) and small jumps are cheaper and safer. You get incremental improvements because it takes a long time to work out the kinks in the big jumps. And you get incremental improvements because you have to focus on improving one part at a time in these complex systems where all the components depend on each other to perform their best.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/eric_he" target="_blank">eric_he</a>
			<div class="markdown"><p>This is a great answer, thanks!</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/duck407" target="_blank">duck407</a>
			<div class="markdown"><p>I work in the semiconductor industry, but with cellphones instead of computers. You have to come out with a new product every year and in that year long cycle you only have about 3-4 months to really design the new part of your next product. In the background there is manufacturing R&amp;D working on new processes, but ultimately for a yearly release there isn't much time to innovate. </p>
<p>It also comes down to money. It's riskier and more expensive to make a huge leap in-between better generations. </p>
<p>As far as making those leaps: usually every year we come up with ways to make things 10-20% better and in that process may discover a way that <em>may</em> give us 30%. With that we work in the background and say &quot;that's for next generation&quot;. It may make it to next gen or it may make it in 2-3 generations from now, but in your current generation you just don't have the time to devote to it. Then over the years you accumulate these &quot;that's for next gen&quot; and this is where the progress comes from. Every year there should probably be a &quot;that's for next gen&quot; and then next gen rolls around, you get your performance bump, and probably think of something for the <em>next</em> gen.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/unkinected" target="_blank">unkinected</a>
			<div class="markdown"><p>Everything you described is the nature of human learning. On a rare occasion, we do make leaps forward in understanding, but the vast majority of time, human advancement is the result of slow incremental improvements as we just keep trying and trying something until we succeed. We tweak little things here and there to improve, because it’s very hard for us to come up with brand new ideas all the time.</p>
<p>Most of the science world’s “innovations” that lay people believe were revolutionary breakthroughs were actually just incremental improvements on pre-existing ideas. Even Einstein’s theories were not out of the blue - they were logical steps from previous researchers’ work.</p>
<p>So in addition to everything else people said here about the annual product cycle and competition, the slow and steady increase in CPUs is because of the way humans (and all animals) learn.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/alanharker" target="_blank">alanharker</a>
			<div class="markdown"><p>I think another way of putting this is lets say graphene is our next leap forward in semicondutor material, and it can hypothetically take us to 1nm transistor size. </p>
<p>So we (again hypothetically, assume this is the case for all that follows) know the material exists and is capable of it. The next challenge is how do we make a platter- any platter- of it? Add months of research finding ways. </p>
<p>Next, we know that its possible to do, great. Now, how do we do this at scale in an industrial setting, what machines and processes can we use? Add more time and money for this.</p>
<p>Next, how do we etch or otherwise create circuitry- any circuitry- on it?</p>
<p>Next, how do we do that at the resolution we need? Do we need to perfect the process on 3nm and shrink to 1nm? </p>
<p>Next, how do we do this at scale and in a cost-effective fashion.
Next, how do our current circuits translate onto this new medium, do they work right away or do we need to reinvent some things.</p>
<p>These are largely sequential processes, work on the next step cant start until we answer the last question.
What is happening in parallel to this is those who have silicon wafers are continually refining their own processes by following similar steps. </p>
<p>So what happens is whilst great leaps are going on, they arent occurring in a vacuum, and by the time they hit the market theyre often not so great any more, or else are priced so expensively that they are restrictive or prohibitive to parts of, or all of, the market. They simply arent viable until the R&amp;D costs are repaid and/or they can hit scale production and the price floor drops away.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/jvgeli62321" target="_blank">jvgeli62321</a>
			<div class="markdown"><p>Correct. There is no “let’s start this in January so we can release this next year” planning for those chip technologies, some research takes years/decades even before they can be applied to production. Some people just don’t realize how all this really is driven by research. The breakthroughs like Ryzen may have been conceived a long time ago.</p></div>		</li>
					</ul>
	