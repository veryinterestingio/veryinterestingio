	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/AngryCaper" target="_blank">AngryCaper</a>
			<div class="markdown"><p>Ok, that video helped a whole lot, thank you very much for that link.  Let me see if I understand it correctly.</p>
<p>You used a technique to be able to subtract one image from another, knowing the main thing that would be left in the subtraction would be light from an exoplanet.</p>
<p>It only takes a 2 second exposure, so really fast for astronomy, but it has a ton of background noise from the camera (like your cell phone camera in really dark conditions with all those random pixels of noise and nothing comes out clear)</p>
<p>You take a long video of the region and end up with a cloud of constantly changing noise kind of like a hours long video of TV static for a dataset.  The noise is completely random, but somewhere within that noise is a pixel that happens more often in the same spot, and that's the exoplanet data.  So by taking the whole video and looking for that pixel that stays in one spot more often you can extrapolate a single picture from the whole video.</p>
<p>Is it safe to look at it as a single pixel with a massive amount of motion blur applied to it during the process to remove it from the background noise?  Is that why it seems to have an odd elongated shape instead of being round, or is the odd shape going to be more of a different random blob every time you make one of these composite images?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/k-wagner" target="_blank">k-wagner</a>
			<div class="markdown"><p>This is a great summary! I’m not supposed to start answering at large yet but I just want to chime in and say that this is essentially spot on until the point about single pixels and motion blurs. An image of a star (or a planet, or anything unresolved) takes up multiple pixels with the way that the optics are aligned. See this article for some extra info: <a href="https://en.wikipedia.org/wiki/Point_spread_function" target="_blank">https://en.wikipedia.org/wiki/Point_spread_function</a></p>
<p>Now, as to why the candidate isn’t exactly circularly symmetric: that can be explained in a few ways. Of course, if it’s just a systematic artifact that we don’t know about, then we also don’t know what shapes to look out for. If it’s a disk of dust around Alpha Cen A, then the elongation also makes sense. If it’s a planet in a ~1 year orbit, then over the few weeks of our observations the planet would cover about 10% of its orbit, which is also consistent with the amount of elongation.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/AngryCaper" target="_blank">AngryCaper</a>
			<div class="markdown"><p>ok, so the Point Spread Function means that even if the 2 suns weren't in the way and you could actually do a long exposure, it wouldn't be a single pixel anyway, it would create a glowing circle of multiple pixels just from the optics of the telescope?  I always assumed that the pixel density on the sensor chip would have been the limiting factor in getting a clear image, but it makes sense the optics would be the limiting factor.  I mean if they can do 12Megapixels on those tiny iPhone camera chips I can just imagine how insanely high they are up to with astronomy digital sensors.</p></div>		</li>
					</ul>
		</ul>
		</ul>
	