	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/esaltz" target="_blank">esaltz</a>
			<div class="markdown"><p>Thanks for this important question. It's a tricky one, that reveals how much questions of &quot;preventing or slowing the spread of misinformation&quot; assume clear definitions of what is and isn't &quot;misinformation,&quot; which can be a deeply social, values-based question, dependent on the institutions and methodologies you trust. These are <a href="https://en.wikipedia.org/wiki/Wicked_problem" target="_blank">“wicked,”</a> sociotechnical problems, and it can be hard to decouple what are the problems we’re seeing that are social and societal problems independent from the role of platforms and media in incubating and amplifying these problems.</p>
<p>The way I approach this as a user experience researcher is to first understand what are the cues that people use to understand what information/sources/narratives are credible or not, and why?  In past user research for the News Provenance Project at New York Times, <a href="https://open.nytimes.com/how-do-people-decide-whether-to-trust-a-photo-on-social-media-e0016b6080ae" target="_blank">we created a framework</a> that considers two factors: trust in institutions (attitude) and attention (behavior) as important determinants of someone’s response to media and receptivity to misinformation narratives. It’s worth appreciating that many people (even those subscribing to conspiracy theories) see themselves as well-meaning and critical consumers of information, especially related to health information which so directly impacts their own and other’s lives. This criticality can be warranted: As others have pointed out, even findings of peer-reviewed scientific papers may not be valid or reproducible, and what is accepted credible scientific wisdom one day may change the next. If there’s one thing we can be sure of, human knowledge is fallible: so building trust means ensuring accountability and correction mechanisms, and mechanisms for citizens to question and engage with data firsthand.</p>
<p>What all this means is that “deal[ing] with folks who've bought massive quantities of misinformation” might mean, on one hand, addressing behaviors: specifically, the distracted, emotional, and less critically engaged modes of information consumption of platforms (for example using recommendations in our post to <a href="https://medium.com/swlh/it-matters-how-platforms-label-manipulated-media-here-are-12-principles-designers-should-follow-438b76546078" target="_blank">“Encourage emotional deliberation and skepticism”</a> while making credible, relevant information easy to process.) On the other hand, it means dealing with trust in institutions, which often relates to deep social/societal ills.</p>
<p>It’s my personal belief that while there are many easy, short-term steps to help mitigate the harmful and divisive effects of aspects of our information environments (and the political entrepreneurs who capitalize on platform dynamics), it’s important to recognize these attitudes form in reaction to social phenomenon and might be rooted in valid feelings and concerns. Some of the approaches I’m most excited about in this area consider modes of <a href="https://misinforeview.hks.harvard.edu/article/repress-redress-what-the-war-on-terror-can-teach-us-about-fighting-misinformation/" target="_blank">“redressing” not “repressing” misinformation.</a></p>
<p>In my current research at the Partnership on AI with First Draft, we’re conducting extensive interviews and in-context diary studies to better understand how these attitudes and behaviors related to COVID-19 information, specifically, so stay tuned!</p></div>		</li>
					</ul>
	