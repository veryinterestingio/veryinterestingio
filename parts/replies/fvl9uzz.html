	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Mukagas" target="_blank">Mukagas</a>
			<div class="markdown"><p>Cant imagine how much shit is going on while I'm playing a game like RDR2</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/DeadlyTissues" target="_blank">DeadlyTissues</a>
			<div class="markdown"><p>Been watching videos on dwarf fortress. That shit is bananas, they program literally every tiny little detail they can, and since it's ascii based they won't deal with any sort of weird modeling or rendering issues. Enables them to get as specific as calculating the fat percentage of a certain character based on a number of factors such as their &quot;genetics&quot; from parents, and their diet from the last 30 years of the simulation. All this only to calculate how combustible this character should be compared to his peers. Another example i loved is how they implemented &quot;tissues&quot; in creatures, with fat being a sub-class of that. Each tissue has its own properties (imagine things like tensile strength, burning points, etc) which are then used to construct the &quot;model&quot; for the character as they are getting sliced through with a sword. Sword should stop at bone unless it's either really sharp or you're really strong. Other games really just fudge these details and make it similar to reality, dwarf fortress has definitely created its own reality from scratch.</p>
<p>What's even crazier to think about is that they were able to develop in this way from the start in 2002, not at all needing the computing resources we have today. Just kinda shows how much of game processing really boils down to rendering and displaying graphics in real time.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/michael_huntertz" target="_blank">michael_huntertz</a>
			<div class="markdown"><p>Wait I played dwarf fortress for years and never knew that....time to reinstall and play again...</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/JohnFromEPA" target="_blank">JohnFromEPA</a>
			<div class="markdown"><p>Link to vids?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/GoldDog" target="_blank">GoldDog</a>
			<div class="markdown"><p>Simple way of getting a ballpark is looking at the specs of your graphics card. A modern graphics card easily does lots and lots of GHz. That's billions of decimal calculations... Per second...so every single second you're watching those horse testicles dangling it's doing more than 1000 000 000 operations... The insane thing is how rarely shit glitches</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/TheAgentD" target="_blank">TheAgentD</a>
			<div class="markdown"><p>You're actually off by 4 orders of magnitudes.</p>
<p>GPUs generally run at under 2 GHz (1.35GHz for an RTX 2080 Ti), quite a bit slower than modern CPUs. They compensate for this by having literally thousands of cores (4352 in an RTX 2080 Ti). In many common cases, each core can do 2 operations per clock cycle (a*b + c counts as one operation).</p>
<p>All in all, the theoretical performance is 1 350 000 000 Hz X 4352 cores X 2 = 11 750 400 000 000, or 11.75 TRILLION floating point operations per second. This number is listed as 11.750 TFLOPS in the specs of the RTX 2080 Ti</p></div>		</li>
					</ul>
		</ul>
		</ul>
	