	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/profdc9" target="_blank">profdc9</a>
			<div class="markdown"><p><em>Basically the human genome project put a million piece (total guess) jigsaw puzzle together without the picture on the front of the box. That was slow and expensive to do. Now that we know what the puzzle should look like, a computer can solve the puzzle in basically no time at all.</em></p>
<p>This is a great analogy and should be at the top.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/knavillus" target="_blank">knavillus</a>
			<div class="markdown"><p>It is a great analogy, and if you refine it further to imagine assembling the text of a 3 billion character book by overlapping random 300-letter fragments you get a clearer picture of the effort. The initial assembly required a lot of overlap in order to identify when two pieces were adjacent. There is also a lot of repetition in the genome, so imagine you also have duplicated chapters with subtle changes to throw you off, as well as long stretches of a single phrase over and over again.  These long repeats are really hard to characterize when your read length is much lower than the repeat length.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/pandizlle" target="_blank">pandizlle</a>
			<div class="markdown"><p>These mapping programs still require supercomputers and hours of time even for just a single chromosome. God, memories of my time in bioinformatics are coming up. Me screaming at the supercomputer for rejecting my data just 30 mins short of completion because I didnâ€™t guess the right amount of time I needed to book it for... Sweet memories.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NumberDodger" target="_blank">NumberDodger</a>
			<div class="markdown"><p>You're right, 'no time at all' is majorly inaccurate for a while genome. Sorry everyone, I was thinking relative to the human genome project!</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NeuralParity" target="_blank">NeuralParity</a>
			<div class="markdown"><p>Computing power has also improved massively in the last 20 years. My group is now routinely running our entire analysis pipeline (qc, mapping, variant calling, ..., oncology patient report), on \~130x coverage worth of whole genome sequencing data for under USD$40 in cloud compute cost.</p>
<p>Compute cost has come down so much that the cost is now all in the data storage. It's getting so expensive that I'm hearing rumours that some groups are just throwing away their sequencing data once they've done their variant calling since it cheaper to resequence the samples that they need to reanalyse from scratch than to keep all their data around for years on end.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/knavillus" target="_blank">knavillus</a>
			<div class="markdown"><p>The read lengths of that era on capillary electrophoresis machines like the ABI 3700 were 300-500 bases, so I think your ball park estimate is on the low side but right order of magnitude. Plenty of overlap was needed and redundancy was high because of the random nature of the sequencing effort.</p></div>		</li>
					</ul>
	