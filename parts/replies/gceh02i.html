	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CinemaAudioNovice" target="_blank">CinemaAudioNovice</a>
			<div class="markdown"><p>This may be a dumb question, but looking at that picture, why are the ceilings so high? Doesn’t that mean there is a bigger volume of space to cool? What am I missing?</p>
<p>Edit: all your replies make total sense, thanks all!</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/almaroni" target="_blank">almaroni</a>
			<div class="markdown"><p>Cool air sinks, hot air rises? :D Pretty simple concept. besides that, it could be due to other technical factors.</p>
<p>9 Feet is the minimum height for ceilings for good heat dispersion and the higher the ceiling the more time you have to repair it if the cooling solution of the datacenter shuts down.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ja5143kh5egl24br1srt" target="_blank">ja5143kh5egl24br1srt</a>
			<div class="markdown"><p>Also that it's a common design for a warehouse and this doesn't even look like that high of a ceiling. Might be the lens.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/y_gingras" target="_blank">y_gingras</a>
			<div class="markdown"><p>There is a lot of ways to do it.  A very common design is raised floors where the cold air is flowing in the floor and they put open tiles in the front of the servers.  The hot air exits at back then mixes with the ambient room air, which is then recycled/chilled down again.   With that design, you have hot aisles and cold aisles, which may be relatively tightly sealed depending on how much efficiency you need.  The cooling efficiency is relative to humidity, so some locales are more forgiving than other when designing a data center cooling system.   If you ever get the chance to visit a server room like that, you will notice how incredibly loud that place is.  There are hundreds of fans blasting full speed 24/7 on every rack.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Diggitalis" target="_blank">Diggitalis</a>
			<div class="markdown"><p>Those raised floor tiles are kinda heavy, too (they're usually metal).  </p>
<p>One time, in the NOC where I worked, we had a server that kept mysteriously overheating, so some overpaid Mensa applicant had the idea of pulling up one of the raised tiles and running a power cord down there for a big oscillating fan, and they loosely replaced the tile so that it was barely askew, with just enough room for the power cable running up through the gap.</p>
<p>Sure enough, that heavy-ass metal tile with kind of sharp edges got knocked back into its proper place, which severed the power cord to the fan and immediately shorted out the circuit, which just so happened to power the entire cooling system for the NOC, so they turned 1 overheating server into an entire room full of them.</p>
<p>Brilliant!</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/SVXfiles" target="_blank">SVXfiles</a>
			<div class="markdown"><p>The headend that runs the spectrum services for my local area of about 25kish people could be a small efficiency apartment in terms of size. There's 2 huge batteries for backup incase power goes down and iirc the guy who ran it said the main power infeed was upwards of 400v. Standing next to someone and shouting was still too quiet to hear what they were saying</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/teh_fizz" target="_blank">teh_fizz</a>
			<div class="markdown"><p>Google is building centers in the north of Sweden and they are using the natural cold as a cooking mechanism. I wonder if they can recycle the heat to heat homes in the area.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/milkysniper" target="_blank">milkysniper</a>
			<div class="markdown"><p>They’re not cooling the space, they’re cooling the servers.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Cronerburger" target="_blank">Cronerburger</a>
			<div class="markdown"><p>Maybe they should cool the users, youtube comments are meltdown critical</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/vwlsmssng" target="_blank">vwlsmssng</a>
			<div class="markdown"><blockquote>
<p>metric fucktons</p>
</blockquote>
<p>metric fucktonnes</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Demonic_Toaster" target="_blank">Demonic_Toaster</a>
			<div class="markdown"><p>Indefinite units of measurement: buttload is slightly more than a shitload, but way less than a fuckton.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/PM_me_your_fav_poems" target="_blank">PM_me_your_fav_poems</a>
			<div class="markdown"><p>Actually, a buttload is a definite unit of measurement. 126 Gallons of wine to be precise.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/sirociper" target="_blank">sirociper</a>
			<div class="markdown"><p>Mighty Mighty Fucktones</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/vwlsmssng" target="_blank">vwlsmssng</a>
			<div class="markdown"><p>Are you thinking of the Might Mighty Bosstones?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NZitney" target="_blank">NZitney</a>
			<div class="markdown"><p>Found converter_bot</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/flif" target="_blank">flif</a>
			<div class="markdown"><p>Let me guess: 2 * 50 racks in each row, 50 rows = 5000 racks.</p>
<p>6 harddrives in each 1U tray, 50 trays in rack = 300 harddrives per rack</p>
<p>Total 1.5 million harddrives. Times 10 Tb per drive = 15 billion Gb.</p>
<p>If one movie is 200 Mb in total for all formats, this datacenter could store be 75 billion movies.</p>
<p>Google: 500 hours of video were uploaded to YouTube every minute</p>
<p>= 263 million hours per year, which is 1.5 billion movies of 10 minutes each.</p>
<p>So this datacenter has plenty of space.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/sfratini" target="_blank">sfratini</a>
			<div class="markdown"><p>This is a good first approach but you are forgetting a few things. Like not all drives are replaced whenever a larger version becomes available do I don’t think all of them are 10 tb. Are also you are forgetting about redundancy. I asume they use raid 5 + 1. So that means drives are about 50% utilized. And also some of these are most likely duplicated in a second data center far away. So I would be surprised if every data center can hold a third of what you said first. But the math checks out.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/undertakersbrother" target="_blank">undertakersbrother</a>
			<div class="markdown"><p>And the upgrade may require 3u instead 1u thus taking up that much more space in the rack.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/marcan42" target="_blank">marcan42</a>
			<div class="markdown"><p>Google hasn't used RAID for over a decade. RAID is useless for cloud storage, because if the server dies all the data dies with it, RAID or no RAID. You need something where if a server dies, or even if a whole rack of servers dies, no data is lost.</p>
<p>Instead the servers themselves don't use any kind of RAID, the drives are basically JBOD. Higher level software takes care of doing stuff like RAID, but way more advanced, across the entire datacenter. The data overhead might be anywhere from 1.2x to 3x depending on how reliable you need it to be vs. how fast vs how efficient.</p>
<p>That said, since an entire datacenter can go offline too for many reasons, Google will pretty much always keep a copy of everything in at least two places.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/andyspantspocket" target="_blank">andyspantspocket</a>
			<div class="markdown"><p>Check out E1.L form factor.  32 drives per 1U rack.</p>
<p>Most racks are 42U, though you can get 21, 36, 48, and 54 pretty easily.</p>
<p><a href="https://en.wikipedia.org/wiki/Enterprise_%26_Data_Center_SSD_Form_Factor" target="_blank">https://en.wikipedia.org/wiki/Enterprise_%26_Data_Center_SSD_Form_Factor</a></p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Calldean" target="_blank">Calldean</a>
			<div class="markdown"><blockquote>
<p>6 harddrives in each 1U tray</p>
</blockquote>
<p>Disk storage tends to not be 1U stuff; it's stacked differently.</p>
<p>EDIT: <a href="https://knowledge.hitachivantara.com/Documents/Storage/VSP_G1X00_and_VSP_F1500/80-06-6x/Product_Overview/02_Hardware_features_of_VSP_G1000%2C_G1500%2C_and_VSP_F1500" target="_blank">Example</a></p></div>		</li>
					</ul>
		</ul>
	