	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/manuelgcg" target="_blank">manuelgcg</a>
			<div class="markdown"><p>Awesome explanation, thanks!</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/18bees" target="_blank">18bees</a>
			<div class="markdown"><p>A fun side note, there’s nothing special about p equaling .05. The inventor, Ronald fisher just decided that number was ‘pretty good’ and set it as the standard!</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/seiferalmasay" target="_blank">seiferalmasay</a>
			<div class="markdown"><p>It's not completely arbitrary like your explanation implies. 95% of data falls within 2 standard deviations of the mean on a normal curve, so a p of 0.05 means your result more than twice as far away from average as you would expect from random chance. This doesn't mean it's impossible, just unlikely.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/splitSeconds" target="_blank">splitSeconds</a>
			<div class="markdown"><p>Yep. And here's another interesting thought. If you get a big enough sample, ANYTHING will be significant.</p>
<p>For example - if you actually were able to get a sample size of the entire world population (EVERYBODY) the correlation between eating carrots and being a genius would be an extremely low p-value (0.00000000001***) and statistically significant.</p>
<p>The correlation R would be stupidly non-useful. But it would be significant. Food for thought.</p>
<p>Also - bouncing off of u/18bees comment, you can think of 0.05 as a convention. Technically, there is nothing wrong with saying something is p &lt; 0.346 which mean's there's less chance of it being due to random chance than say p = 0.348. But since we are creatures that better understand things on consistency ... 0.05, 0.01, 0.001 etc. have become somewhat accepted markers.</p>
<p>A related note to this: You might see some people claim things like a p = 0.06 as:</p>
<ul>
<li>approaching significance</li>
<li>close to significant</li>
<li>almost significant</li>
</ul>
<p>Different fields have different feelings about this sort of framing. I tend to think it's a dangerous slope to play on. I'd rather just people say something like p &lt; 0.10 than try to make something like a 0.06 out to be below the conventional threshold. (I find that when I hear people doing this sort of thing, they started their analysis with the standard p &lt; 0.05 then &quot;shifted&quot; their goalposts so they could talk about something with more &quot;marketing&quot; power.)</p>
<p>You can still talk about things and inquire about what they mean even if something isn't significant. This is especially crucial for the discourse when you have a lot of evidence that would have suspected something and it didn't meet the expectation - then talk about why this might have happened - weigh in on whether we need to rethink how we tested, other factors that might be in play, etc. But this whole publishing just significant findings is really a bad trend in research.</p>
<p><em>Things are statistically significant, or they are not, based on conventions you agree to before you go about analyzing the data.</em></p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/autostclair" target="_blank">autostclair</a>
			<div class="markdown"><p>Yep. Most medical studies require p-values of 0.01 because... maybe we should be EXTRA sure before cutting people open or injecting lab-created substances into them, y'know?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/striatedgiraffe" target="_blank">striatedgiraffe</a>
			<div class="markdown"><p>To add on to the explanations. The desired p value also depends on what field you work in too. So you've been told that a p value of 0.05 corresponds to 2 standard deviations of significance. In particle physics we actually need to have 5 standard deviations of significance to claim discovery which is a p value of 3×10^-7.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rpcleary" target="_blank">rpcleary</a>
			<div class="markdown"><p>Possibly interesting read for you about it and why it can be so hard to explain/understand:</p>
<p>&#x200B;</p>
<p><a href="https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/" target="_blank"><a href="https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/" target="_blank">https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/</a></a></p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rlbond86" target="_blank">rlbond86</a>
			<div class="markdown"><blockquote>
<p>A p value of 0.05 means there is a 5% chance that the results you got support your conclusion not because there is a bias for some reason towards such results (say a coin is heaver on one side than another) but because of just dumb luck. </p>
</blockquote>
<h1>This is wrong!</h1>
<p>A p-value of 0.05 means that <em>if</em> there was really no effect at all, you'd see a result this strong 5% of the time. It's a very important difference. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/overzealous_dentist" target="_blank">overzealous_dentist</a>
			<div class="markdown"><p>I'm reading both of those explanations as the same thing. What's the difference?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/2074red2074" target="_blank">2074red2074</a>
			<div class="markdown"><p>Let's say you think there's a link between diet and cancer. You test 1000 different foods. You should expect 50 of those to get p&lt;0.05 just because of random chance, and one of those is actually the food that causes the cancer. So you have a 2% chance of it being a coincidence even though p&lt;0.05.</p>
<p>To use an easier example, let's say 1% of people have ass cancer. You have a test with 95% accuracy. If 1000 random people take this test, you'd expect the 10 people with ass cancer to test positive. But of the 990 other people, 50 of them will test positive due to random chance. So if a given person tested positive for ass cancer on a 95% correct chance, there's only about a 1/6 chance that they actually have ass cancer.</p>
<p>In other words, there is an insanely high number of things that are not correlated to whatever you're testing. So when p&lt;0.05, it's still probably just a false positive. If you find p=.05, that doesn't mean there's a 95% chance you're right.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rlbond86" target="_blank">rlbond86</a>
			<div class="markdown"><p>So you're testing a drug. In reality it's a placebo, but you don't know that. You perform a test and the p-value is 0.05.</p>
<p>Does that mean there's a 95% percent chance that your drug has an effect and a 5% chance it's a placebo? No (in fact there's a 100% chance it's a placebo! You just don't know that). It means that you got a result that you would get from a placebo 5% of the time.</p>
<p>Now re-read the paragraph above, but this time you have a drug that in reality <em>is</em> more effective than a placebo. You do a test and get a p-value of 0.05.</p>
<p>The same thing is true as above. It's not that there's a 5% chance it's a placebo (in fact there's a 0% chance it's a placebo but you don't know that). It's that you got the same result that you'd get 5% of the time if you had tested a placebo. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/lastsynapse" target="_blank">lastsynapse</a>
			<div class="markdown"><p>Basically the idea is that this is the probability that you obtained this score or statistic with no effect. So low means unlikely.  High p values are likely. What is being tested is the probability that you would get data that look like that if there was no effect.</p>
<p>Given that you have one dataset to look at, you have to choose: is my data like this because there isn’t an effect or is my data like this because there is an effect. </p>
<p>Since this probability is low (p&lt;0.05), you conclude it is more likely you would always get data like this, consistent with the logic that there was an effect and inconsistent with the logic that there was not an effect. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NoJster" target="_blank">NoJster</a>
			<div class="markdown"><p>Thank you for pointing this subtle, but highly important, fact out!</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/lastsynapse" target="_blank">lastsynapse</a>
			<div class="markdown"><p>I cannot upvote this enough. The top explaination is fundamentally misunderstanding the null hypothesis test that is implied. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Ufarious" target="_blank">Ufarious</a>
			<div class="markdown"><p>A P-Value does <em>not</em> tell you the likelihood that your outcome is due to chance. It is simply the probability that given a true null-hypothesis, you will see results at least this extreme. It is not the error rate for your test. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/hmt28" target="_blank">hmt28</a>
			<div class="markdown"><p>The p-value relating to chance has to be the biggest misconceptions about interpreting p-values. </p>
<p>It’s hard for me to remember, so I constantly use <a href="https://www.amstat.org/asa/files/pdfs/p-valuestatement.pdf" target="_blank">this</a> reference. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Ufarious" target="_blank">Ufarious</a>
			<div class="markdown"><p>Agreed. This excerpt from the <a href="https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/" target="_blank">Five Thirty Eight article</a> referenced in this thread is a good example.</p>
<blockquote>
<p>Imagine . . . that you have a coin that you suspect is weighted toward heads. (Your null hypothesis is then that the coin is fair.) You flip it 100 times and get more heads than tails. The p-value won’t tell you whether the coin is fair, but it will tell you the probability that you’d get at least as many heads as you did if the coin was fair. That’s it — nothing more. </p>
</blockquote>
<p>&#x200B;</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Cookie136" target="_blank">Cookie136</a>
			<div class="markdown"><p>I mean it does still relate to chance. It's just specifically the chance you would obtain these results assuming the null hypothesis is true.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/elego" target="_blank">elego</a>
			<div class="markdown"><p>A good illustration of this is Simpson's paradox. It goes like this.</p>
<p>We all know the common explanation of a p-value, trying to determine whether a coin is fair or not:</p>
<ol>
<li>Null hypothesis is you have a fair coin</li>
<li>You observe 100 heads in a row</li>
<li>Given the null hypothesis being true, it's extremely unlikely to observe 100 heads (p &lt; .05)</li>
<li>Therefore you reject the null--i.e. it's not a fair coin</li>
</ol>
<p>Here's the paradox-y part. This is logically the same as the following. You have sampled a random person (Bill) and suspect he may not be an American, and want to be sure:</p>
<ol>
<li>Null hypothesis is Bill is an American</li>
<li>You observe that Bill is a US Senator</li>
<li>Given the null hypothesis being true (Bill is an American), it's extremely unlikely for him to be a Senator</li>
<li>Therefore Bill's not an American</li>
</ol>
<p>Obviously this is nonsensical since we've already observed that Bill is a US Senator, and therefore an American citizen.</p>
<p>The problem (I think) comes down to prior probabilities, which p-values don't take into account, but something like Bayesian inference would.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Automatic_Towel" target="_blank">Automatic_Towel</a>
			<div class="markdown"><p>Is this <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox" target="_blank">Simpson's paradox?</a> I think it's <a href="https://en.wikipedia.org/wiki/Confusion_of_the_inverse" target="_blank">conditional probability fallacy.</a> Or perhaps related to <a href="https://en.wikipedia.org/wiki/Lindley%27s_paradox" target="_blank">Lindley's paradox?</a></p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/T-T-N" target="_blank">T-T-N</a>
			<div class="markdown"><p>If you're completely barking up the wrong tree, what is the chance that you get your results</p></div>		</li>
					</ul>
		</ul>
	