	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/collegiaal25" target="_blank">collegiaal25</a>
			<div class="markdown"><blockquote>
<p>At p=0.17, it's still more likely than not than the die is weighted, </p>
</blockquote>
<p>No, this is a common misconception, the base rate fallacy.</p>
<p>You cannot infer the probablity that H0 is true from the outcome of the experiment without knowing the base rate.</p>
<p>The p-value means P(outcome | H0), i.e. the chance that you measured this outcome (or something more extreme) assuming the null hypothesis is true.</p>
<p>What you are implying is  P(H0 | outcome), i.e. the chance  the die is not weighted given you got a six. </p>
<p><strong>Example:</strong></p>
<p>Suppose that 1% of all dice are weighted The weighted ones always land on 6. You throw all dice twice. If a dice lands on 6 twice, is the chance now 35/36 that it is weighted?</p>
<p>No, it's about 25%. A priori, there is 99% chance that the die is unweighted, and then 2.78% chance that you land two sixes. 99% <em> 2.78% = 2.75%. There is also a 1% chance that the die is weighted, and then 100% chance that it lands two sixes, 1% </em> 100% = 1%. </p>
<p>So overal there is 3.75% chance to land two sixes, if this happens, there is 1%/3.75% = 26.7% chance the die is weigted. Not 35/36= 97.2%.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Astrokiwi" target="_blank">Astrokiwi</a>
			<div class="markdown"><p>You're right. You have to do the proper Bayesian calculation. It's correct to say &quot;if the dice are unweighted, there is a 17% chance of getting this result&quot;, but you do need a prior (i.e. the rate) to properly calculate the actual chance that rolling a six implies you have a weighted die.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/collegiaal25" target="_blank">collegiaal25</a>
			<div class="markdown"><blockquote>
<p>but you do need a prior</p>
</blockquote>
<p>Exactly, and this is the difficult part :)</p>
<p>How do you know the a priori chance that a given hypothesis is true? </p>
<p>But anyway, this is the reason why one should have a theoretical justification for a hypothesis and why data dredging can be dangerous, since hypotheses for which a theoretical basis exist are a priori much more likely to be true than any random hypothesis you could test. Which connects to your original post again.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ExistentialDuck" target="_blank">ExistentialDuck</a>
			<div class="markdown"><p>I think you addressed the absence of a prior well in your original comment stating one should calculate the odds of any of the 10,000 getting multiple 6s in a row (since the prior is unknown in any given study) The bayesian discussion didn't serve to change the validity of your original comment, but it did expand upon the conversation in a way that made me think and I appreciate you both being so forthcoming with your knowledge.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Baloroth" target="_blank">Baloroth</a>
			<div class="markdown"><p>You don't need Bayesian calculations for this, you just need a null hypothesis, which is very different from a prior. The null hypothesis is what you would observe if the die were unweighted. A prior in this case would be how much you believe the die is weighted <em>prior</em> to making the measurement. </p>
<p>The prior is needed if you want to know, given the results, how <em>likely</em> the die is to actually be weighted. The p-value doesn't tell you that: it only tells you the probability of getting the given observations if the null hypothesis were true. </p>
<p>As an example, if you <em>know</em> a die is fair, and you roll 50 6s in a row, you'd still be sure the die is fair (even if the p-value is tiny), and you just got a very improbably set of rolls (or possibly someone is using a trick roll).</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Cmonredditalready" target="_blank">Cmonredditalready</a>
			<div class="markdown"><p>So what would you call it if you rolled all the dice and immediately discarded any that rolled 6? I mean, sure, you'd be throwing away ~17% of the good dice, but you'd eliminate ALL the tampered dice and be left with nothing but confirmed legit dice.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/kpengwin" target="_blank">kpengwin</a>
			<div class="markdown"><p>This really leans into the assumptions that a tampered die will 100% of the time roll 6 - whether this is reasonable or not would presumably depend on variables like how many tampered dice there actually are, how bad it is if a tampered die gets through, and whether you can afford to loose that many good dice. In the 100% scenario, there's no reason not to keep rolling the dices that show 6s until they roll something else, at which point it is 'cleared of suspicion.'  </p>
<p>However, in the more likely real world scenario where even tampered dice have a chance of not rolling a 6, this thought experiment isn't very helpful, but the math listed above still will work for deciding if your dice are fair.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MrFanzyPanz" target="_blank">MrFanzyPanz</a>
			<div class="markdown"><p>Sure, but the reduced problem he was describing does not have a base rate. It’s analogous to being given a single die, being asked whether it’s weighted or not, and starting your experiment. So your argument is totally valid, but it doesn’t apply perfectly to the argument you’re responding to.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/collegiaal25" target="_blank">collegiaal25</a>
			<div class="markdown"><p>For many hypotheses we don't have a base rate. That is what makes it so extremely difficult to  tell the chance that a hypothesis is true or not.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Wolog2" target="_blank">Wolog2</a>
			<div class="markdown"><p>&quot;This 0.17 is the p-value. It is the probability that your result isn't caused by your hypothesis (here, that the die is weighted), and is just caused by random chance.&quot;</p>
<p>This should read: &quot;It is the probability that you would get your result assuming the null hypothesis (that the die is unweighted) were true&quot;</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Zgialor" target="_blank">Zgialor</a>
			<div class="markdown"><p>After rolling a 6, the probability of the die being unweighted would be 1/7, i.e. about 0.14, right? (assuming any given die has a 50% chance of being weighted before you roll it)</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RCIVcm" target="_blank">RCIVcm</a>
			<div class="markdown"><p>This answer gets the flavor of p-hacking right, but commits multiple common errors in describing what a p-value means.</p>
<blockquote>
<p>This 0.17 is the p-value. It is the probability that your result isn't caused by your hypothesis (here, that the die is weighted), and is just caused by random chance.</p>
<p>the probability that some correlation or other result was just a coincidence, produced by random chance.</p>
</blockquote>
<p>No!! The p-value has <em>nothing</em> to do with cause, and in fact says <em>nothing</em> directly about the alternative hypothesis &quot;the die is weighted.&quot; It is not the probability that your data was the result of random chance. It is only and exactly &quot;the probability of my result if the null hypothesis was in fact true.&quot;</p>
<p>The p-value speaks about the alternative hypothesis only through a <em>reductio ad absurdum</em> argument (or perhaps <em>reductio ad unlikelium</em>) of the form: &quot;<strong>if the null hypothesis were true, my data would have been very unlikely</strong>; therefore, I suspect that the null hypothesis is false.&quot; The bolded part corresponds to an experiment yielding a small p-value.</p>
<blockquote>
<p>At p=0.17, it's still more likely than not than the die is weighted if you roll a six</p>
</blockquote>
<p>I'm not certain what this is supposed to mean, but it is not a correct way of thinking about p=0.17.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Dernom" target="_blank">Dernom</a>
			<div class="markdown"><p>I fail to see the difference between &quot;there's a 17% chance that the result is caused by chance&quot; and &quot;there's a 17% of this result if there's no correlation (null hypothesis)&quot;. Don't both say that this result will occur 17% of the time if the hypothesis is false?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RCIVcm" target="_blank">RCIVcm</a>
			<div class="markdown"><p>The phrase &quot;caused by chance&quot; doesn't have a well-defined statistical meaning. We are always assuming that our observation is the outcome of some random process (an experiment, a sampling event, etc.), and in that sense our observation is always the result of random chance; we are just asking whether it was random chance <em>under the null hypothesis</em> or not.</p>
<p>It's unclear to me what &quot;there's a 17% chance that the result is caused by chance&quot; is intended to mean. If it is supposed to be &quot;There's a 17% chance that there is no correlation&quot; (i.e. the probability that the null hypothesis is true is 17%) in your example, then no, the p-value does not have that meaning.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/vanderBoffin" target="_blank">vanderBoffin</a>
			<div class="markdown"><p>One is: &quot;given this hypothesis, what is the probability of getting this data set?&quot;. The other is &quot;given this data set, what is the probability of this hypothesis bring true?&quot;. The p value tells you about the first question, not the second.</p></div>		</li>
					</ul>
		</ul>
		</ul>
	