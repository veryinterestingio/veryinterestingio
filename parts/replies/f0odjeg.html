	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/JamesHeathers" target="_blank">JamesHeathers</a>
			<div class="markdown"><p>Spending money on writing resources which actually help the original authors, rather than returning them blithe comments about 'involve an English speaker in the writing of your manuscript'.</p>
<p>A paper does not have to start off being well written to eventually become well written.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Anon5038675309" target="_blank">Anon5038675309</a>
			<div class="markdown"><p>On the topic of clarity and English, what, if anything, are you doing to address misinterpretation of studies, specifically laypeople and often scientists when they assume a null of convenience, i.e., conclude there is no difference or effect because the study saw no effect? I see it all the time when talking about politically charged issues like GMOs or vaccine safety. An outfit will conduct a study without sufficient statistical power or addressing it in their methods.</p>
<p>They see no difference because, duh, they didn't have the power to resolve the difference if it exists, then report they didn't see a difference. Then idiots conclude science decidedly concluded no difference and are happy to crucify anyone who questions. Even worse, they can have scientific validity and sufficient sample size, but then use the wrong tests. It's like they've gone through the motions of science for so long without thinking about it that a no effect null and confidence of 95% is default or standard, even though it's completely arbitrary, and has dangerous implications when used at scale. Is there anything that can be done?</p>
<p>Do you understand the question? If not, I understand. My dissertation advisor, in spite of his statistical prowess, had trouble. Outside of statisticians, I've only ever met a handful of engineers and MPH folks who get it. It's hard, back to the English thing, when science is conducted in English these days and words like normal, significant, accurate, precise, power, etc. have shitty colloquial meanings. It's also hard when the average person, English or not, isn't well versed in logic or discrete math.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/JamesHeathers" target="_blank">JamesHeathers</a>
			<div class="markdown"><p>Jeez, this is a good one. It's a common enough point among statisticians (or maybe I just talk to them a lot) but it's <em>really</em> hard to communicate.</p>
<p>This one could benefit from some high profile science journalists getting interested in it, honestly. Like you say, it's a semantics issue before it's even an issue about understanding resolving an effect size.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Gastronomicus" target="_blank">Gastronomicus</a>
			<div class="markdown"><p>This sounds like an issue that should be resolved during peer review. But as you note, many people seem to have difficulty grasping the power/null effect aspects of inferential statistics and an over-abundance of confidence in confidence intervals. Journal reviewers and editors need to take a heavy hand in either major edits to, or rejecting, papers that draw spurious conclusions based on mis-interpretation of statistical results.</p></div>		</li>
					</ul>
		</ul>
		</ul>
	