	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/hughperman" target="_blank">hughperman</a>
			<div class="markdown"><p>This was an excellent explanation, thank you very much.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/JTBreddit42" target="_blank">JTBreddit42</a>
			<div class="markdown"><p>Can’t entropy be defined In macroscopic terms?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/forte2718" target="_blank">forte2718</a>
			<div class="markdown"><p>Sort of.  It is often described as the amount of disorder in a system, or a measure of how unavailable internal energy of the system is for doing work with, but these sorts of definitions are only approximate or subjective (for example I can think of systems that appear very ordered and predictable which are very high in entropy, and ones which are very chaotic and unpredictable but very low in entropy).</p>
<p>Thermodynamics emerges as a limit out of the statistical mechanics of quantum particles, so any really good definition of entropy is going to reflect in some capacity how the microscopic details translate over to macroscopic notions of entropy.</p>
<p>Hope that helps,</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/daou0782" target="_blank">daou0782</a>
			<div class="markdown"><blockquote>
<p>for example I can think of systems that appear very ordered and predictable which are very high in entropy, and ones which are very chaotic and unpredictable but very low in entropy</p>
</blockquote>
<p>could you give some examples? i'd really appreciate it.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/stats_commenter" target="_blank">stats_commenter</a>
			<div class="markdown"><p>I mean... you dont need quantum particles at all to define thermodynamics. You can use classical mechanics and get thermodynamics.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CyberTeddy" target="_blank">CyberTeddy</a>
			<div class="markdown"><p>In macroscopic terms, couldn't entropy be defined as &quot;hot&quot; and &quot;cold&quot; and &quot;compressible&quot; and &quot;incompressible&quot;?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/cygx" target="_blank">cygx</a>
			<div class="markdown"><p>In various ways, and from different starting points. For example, there's a rather <a href="https://arxiv.org/abs/cond-mat/9708200" target="_blank">abstract approach via adiabatic accessibility</a>. Sometimes, a simple</p>
<pre><code>S = (U + pV - µN) / T</code></pre>
<p>can suffice.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Insert_Gnome_Here" target="_blank">Insert_Gnome_Here</a>
			<div class="markdown"><p>Is that basically energy per unit temperature or the like?<br />
Because that's what turns up in our thermo lectures.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Goobera" target="_blank">Goobera</a>
			<div class="markdown"><p>Sure, it is a function of the internal energy and external parameters such that it increases monotonically with the internal energy and if an equilibrium state is adiabatically accessible from the initial equilibrium state then the entropy of the second state is more than the initial.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/AxelBoldt" target="_blank">AxelBoldt</a>
			<div class="markdown"><blockquote>
<p>And that's where the logarithmic relationship between thermodynamic entropy and information entropy comes from!</p>
</blockquote>
<p>But I don't think there is such a logarithmic relationship; the two entropies are basically the same thing, except for units. There is a logarithmic relationship between thermodynamic entropy and the number of equally likely microstates that make up a macrostate, however.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/forte2718" target="_blank">forte2718</a>
			<div class="markdown"><p>Sorry, I had mis-typed that sentence without realizing it and another user actually just caught it some minutes ago -- I had meant to say &quot;the microstates and information entropy&quot; not &quot;the thermodynamic and information entropy.&quot; I have already edited it for correctness!</p>
<p>Hope that helps,</p></div>		</li>
					</ul>
		</ul>
	