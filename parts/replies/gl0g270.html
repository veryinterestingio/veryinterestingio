	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mjosofsky" target="_blank">mjosofsky</a>
			<div class="markdown"><p>Thank you for this excellently clear explanation</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/[deleted]" target="_blank">[deleted]</a>
			<div class="markdown"><p>[entfernt]</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Waferssi" target="_blank">Waferssi</a>
			<div class="markdown"><p>I'd say this is the least helpful explanation of the concept of entropy - mainly because of how superficial it is - and I feel like it's mainly used by people trying to sound smart without actually having a clue. </p>
<p>Also, as studying physicist, I'd prefer to say &quot;Entropy is a measure of disorder*&quot;, and I feel like you can't hope to properly explain the concept without mentioning degeneracy of states like u/Weed_O_Whirler did. He even made a quick reference to Boltzmann's entropy formula. </p>
<p>*(even though 'chaos' and 'disorder' are synonyms in standard english, 'disorder' in physics is generally used when discussing static (thermodynamic) systems and entropy, while 'chaos' is used for <a href="https://en.wikipedia.org/wiki/Chaos_theory" target="_blank">dynamic, often mechanical systems</a>.)</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rartrarr" target="_blank">rartrarr</a>
			<div class="markdown"><p>The “how many microstates lead to the same macrostate” from the parent comment is such a much better one-sentence version (precisely quantifiable, not resorting to vagaries, and most importantly, not conflating entropy with the second law of thermodynamics) that there’s not even any comparison. It actually explains what entropy <em>is</em> rather than what it is <em>like</em> or <em>usually invoked to refer to</em>.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/no_choice99" target="_blank">no_choice99</a>
			<div class="markdown"><p>Then why oil and water tend to split nicely over time rather than get mixed chaotically?</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/bert_the_destroyer" target="_blank">bert_the_destroyer</a>
			<div class="markdown"><p>Thank you, this explanation is very clear.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ultralame" target="_blank">ultralame</a>
			<div class="markdown"><p>If I may add a little...</p>
<p>Not only does it require you to expend energy to align those dipoles... If you decrease the entropy of that chunk of iron by 10 units while magnetizing it, the entropy of the entire system (the magnet, your hand, the bio processes you used to move your hand, your breath, and even the information you gained while doing this) must increase by at least 10 units- and in practice, increases by significantly more.</p>
<p>That's the second law... That entropy of the whole system must increase.</p>
<p>If this was not true, then complex systems could spontaneously return to low entropy states. One example of this would be that you could drop an ice cube into a glass of water and the heat could spontaneously  flow out of the ice into the liquid, making the cube colder and the water warmer.</p>
<p>But we know that doesn't happen, so it's a fundamental law.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/severoon" target="_blank">severoon</a>
			<div class="markdown"><p>It's also interesting to take the next step on top of this and explain how spontaneity works. People always make the mistake of thinking that matter will always slide toward a high entropy state by itself, and that any given thing in any given situation will always naturally move to higher entropy.</p>
<p>That isn't true. First, a configuration can be stable. If you think about the iron bar that's been magnetized, that's somewhat stable so that state of being magnetized hangs around for awhile. You could think about a different situation where the configuration is very rigidly locked in, like say the arrangement of atoms in a crystal structure like diamond.</p>
<p>For a configuration to actually move to a higher entropy state, there has to be a <em>pathway</em> available for it to happen. For example, if you measure the entropy of the carbon atoms in diamond, then break the diamond apart and measure the entropy afterwards, it will be higher…but that doesn't mean the carbon atoms will fall apart without you adding a lot of energy. You can think of this as the atoms being in a high energy state in the crystal, wanting to tend toward a lower energy state, but they can't because there is a huge hump in front of them they have to get over akin to &quot;activation energy.&quot; When you come along with a giant sledgehammer and provide that energy, they can get over the hump and achieve that lower energy state. No matter how much you hit the bits, though, the crushed up pieces of diamond will never reform into a whole diamond, they'll just break up further. But the point is just because a state is higher entropy doesn't necessarily mean that state is available in the context of what is happening.</p>
<p>So if the options are stay put or go to higher entropy, both of those outcomes are possible…but what about moving to an even lower entropy state? Yes, it turns out, if you define a system such that energy is being added to it, things can <em>spontaneously</em> move to lower entropy states!</p>
<p>Consider how the diamond formed in the first place. If you define your system to be just those carbon atoms, they weren't always in the form of a diamond. At some point, they were bumping around not in a crystal structure, then something happened, and they were in that structure…entropy decreased. We know from picturing the energy before that they went to a higher energy state; that is, energy was added to this system.</p>
<p>To understand how this happens, imagine a puddle of saltwater. At first, there are salt ions floating around in the water in a very high entropy state, randomly bumping around. As the water evaporates, though, the salt ions have less and less room to bump around and start to form up into highly ordered crystals all by themselves. By the time the water is completely gone, we see that all of the salt has formed itself up into crystals.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Abiogenejesus" target="_blank">Abiogenejesus</a>
			<div class="markdown"><p>Small addition: entropy doesn't <em>have</em> to increase globally, but the odds of global entropy decrease are negligibly small.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Prof_Acorn" target="_blank">Prof_Acorn</a>
			<div class="markdown"><blockquote>
<p>If this was not true, then complex systems could spontaneously return to low entropy states.</p>
</blockquote>
<p>Isn't this essentially what happens when mass starts to coalesce through gravity until a star forms?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/redditshy" target="_blank">redditshy</a>
			<div class="markdown"><p>That was cool, thanks for asking!  I understood the idea before now, but not the how or why.  Fun stuff.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/OkTurnover1898" target="_blank">OkTurnover1898</a>
			<div class="markdown"><p>By the way, the entropy definition is also valid in other fields than physics.</p>
<p>In information system, you can define the entropy of a signal. Defining the entropy can lead to know how much you can compress it. For exemple, an image with random pixels can't be really compressed, however a picture with only 1 color can be compressed a lot. This depend of the algorithme of course!</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/conservexrg" target="_blank">conservexrg</a>
			<div class="markdown"><p>Indeed the same entropy definition, as a mathematical expression, corresponds to the Entropy of a signal in information theory and the Information of the density matrix in quantum physics.</p>
<p>In fact, the connection runs much deeper. While its abstraction may not be, Information itself is inherently physical: the information in an analog signal traversing a noisy channel with Additive White Gaussian Noise, for instance, is a voltage on a wire or an electric field in the air; a bit of digital information, in the case of Flash memory, represents the presence or absence of electrons on the gate of a single transistor.</p>
<p>What we call Entropy in physics often refers to disorder or noise. What we call Entropy in information theory often refers to order or a signal. At the interface of these fields, where I happen to work as a researcher, the former is often called Physical Entropy (PE) while the latter is called Information Theoretic Entropy (ITE).</p>
<p>A real physical system (of finite physical extent and finite high energy cutoff in the particle physics sense) has a finite number of degrees of freedom. Those degrees of freedom maybe be useful information, in which case we call it ITE, or just noise (PE). The second law says we can lose ITE to PE, but they are in a sense the same stuff and measurable in the same units, bits.</p>
<p>Thus we can make odd statements like &quot;All sunshine on Earth corresponds to X bits per second.&quot;</p>
<p>There is a constant flow of PE-type bits incident on the surface of Earth, and a constant flow of PE-type bits back out into the Universe, with the latter greater than the former. That is why we can use solar power to run a computer. ITE_in + PE_in = constant = ITE_out + PE_out, but the ITE_in &gt; ITE_out, so we can use some of it to store and manipulate the information we care about, then throw it away when we're done.</p>
<p>A little trippy if you ask me, but also quite interesting. All the more so as information technology hits the atomic scale and the number of degrees of freedom is small enough that we can no longer neglect this connection.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/sonfer" target="_blank">sonfer</a>
			<div class="markdown"><p>Fascinating. I’ve always heard the universe is in a state of entropy and I always assumed that meant decay. But that’s not true right? If what I understand from your iron example entropy is merely more micro states?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Weed_O_Whirler" target="_blank">Weed_O_Whirler</a>
			<div class="markdown"><p>Well. Sadly, the universe is headed in a direction of high entropy, which there is a reason people consider that decay. </p>
<p>There is another law in thermal physics that in any system, the highest entropy is if that entire system is at the same temperature. So, if you put a hot metal ball and a cold metal ball in an insulated box, they won't stay 1 hot and one cold, but the hot one will cool down and the cold one will heat up until they are the same temperature. This is due to entropy having to increase in a sealed system, and that is the highest entropy result. </p>
<p>Well, if you draw a box around the universe, you will see that it is hot balls (stars) and cold balls (everything else, like planets) and since entropy must increase, that means that eventually the entire universe will be the same temperature. Once the universe is the same temperature, you can no longer do anything useful in it. There's no way to extract energy from one place and put it somewhere else.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/postinthemachine" target="_blank">postinthemachine</a>
			<div class="markdown"><p>I hope you can humour my limited understanding of all this, your previous posts were incredibly interesting. I find the idea of thermal equilibrium if you'd call it that, quite fascinating. If say, in the far distant future our galaxy itself came to a point where it was not receiving light from any other point in the universe, would the galaxy itself eventually reach some point of equilibrium through thermodynamics or would gravity/black holes play a greater role in keeping that system from reaching such a state? I imagine the overall temperature of the universe plays it's own part, not something I can easily wrap my head around.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ChaoticxSerenity" target="_blank">ChaoticxSerenity</a>
			<div class="markdown"><p>Is this what people mean when they talk about the &quot;heat death of the universe&quot;?</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/u777666" target="_blank">u777666</a>
			<div class="markdown"><p>Do we know what caused all of the low entropy states to begin with?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Competitivepublic50" target="_blank">Competitivepublic50</a>
			<div class="markdown"><p>*In the process of writing this I became really skeptical of everything I'm saying so take it with a grain of salt.</p>
<p>If I wanted to make a code to represent you, one shortcut I could take is to use one bit for <em>human</em>/<em>not human</em>. By setting that bit to <em>human</em> I can encode a lot of your state indirectly; like now I don't need a bit for <em>has skin</em>/<em>doesn't have skin</em>.</p>
<p>But someday you will become less structured and I won't be able to take those shortcuts anymore. When I want to encode you I'll have to record a complete state for every single grain of dust that used to be you.</p>
<p>In the beginning we didn't need even one bit to represent everything in the universe: it was all in the singularity and there was no other state it could be in (this is a mental picture not physics)</p>
<p>Eventually the universe will just be an expanse of dust and the number of bits we'll need to encode its state will be maximized. </p>
<p>Anyway I think that's a fair sense of the word <em>decay</em>. But rather than decaying, the information in the universe is going up.</p>
<p>Another way to think about it: if we have two galaxies distant from each other then we can describe one galaxy completely, independent of the other. But as photons from one reach the other, an alien race hears radio messages from earth. We can no longer describe them without describing humans and the influence we had on them. As everything becomes correlated with everything else it becomes impossible to have complete information about something without having information about everything.</p>
<p>*This is just a mental picture not physics</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/platitood" target="_blank">platitood</a>
			<div class="markdown"><p>By recording a single bit for &quot;human&quot;, you are using compression.  In the same way that I could simply write &quot;boat&quot; and avoid providing all the info about a boat.  </p>
<p>However, most compression involves some loss.  For example, without adding back more info, I don't know much about the human or the boat.  The more detail I add, the better I can reproduce the original, but the worse my compression is.  Likewise &quot;human&quot; is a very generic description, and even the &quot;has skin&quot; example may not apply to a burn victim's entire body for example.  </p>
<p>Lossless compression requires that you find information that can be perfectly compressed, because of patterns or repetition.  So yes, the more entropy the less compressible.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Halvus_I" target="_blank">Halvus_I</a>
			<div class="markdown"><p>The 'decay' that you see described is the loss of <em>potential</em>. The universe is trending towards zero potential where nothing can be done because all the energy is at the lowest state and cant move. You need differences for the mechanics of the universe to function.</p></div>		</li>
					</ul>
		</ul>
	