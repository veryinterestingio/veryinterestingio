	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/grandoz039" target="_blank">grandoz039</a>
			<div class="markdown"><p>Worth noting tho that if YouTube didn't use lossy compression, 4k and 1080p options should be equivalent on 1080p PC.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ryan30z" target="_blank">ryan30z</a>
			<div class="markdown"><p>This is kind of redundant, what video streaming service is going to use lossless compression.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/grandoz039" target="_blank">grandoz039</a>
			<div class="markdown"><p>I was mostly making that comment because the way the comment was phrased. While technically correct, it seemed as potentially somewhat misleading. The first sentence + &quot;Throw in data compression on videos and you end up ...&quot; felt to me like someone could read that as &quot;it's caused by x and if you throw in data compression, it gets even worse&quot;, even if that's not what's being explicitly said.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Head_Cockswain" target="_blank">Head_Cockswain</a>
			<div class="markdown"><p>Working top-down, this is the most accurate answer so far.</p>
<p>The stuff about youtube and compression is pretty irrelevant.</p>
<p>The same problem happens with images captured @ display resolution and 3d renders as well. People have been using greater resolutions and then downscaling for a long time.</p>
<p>An image captured at a higher native resolution(4k(what most people mean is UHD, not 4k, but what worked in marketing was &quot;4k&quot; so it stuck)) has exactly four times the data.</p>
<p>When you take that information and do the averaging, you wind up in a situation where the data, averaged out, is better than raw detection at native 1080.</p>
<p>In other words, the processing needed to reduce a 4k to 1080 yields pixel blending that can be more similar to what they eye interprets in real life than a chonky lower resolution captures.</p>
<p>A 9x9 capture grid can yield something approximately round(say a black dot on a white background), but a 3x3 doing the capture will get you a single dark pixel or 9 total grey ones.  Averaging out that 9x9 can yield maybe a dark pixel surrounded by semi dark pixels in the resulting 3x3(or a + or an x shape), when that is a tiny tiny part of a lower resolution display <em>looks</em> like a round dot, almost a sort of illusion.</p>
<p>This is leveraged a LOT by image software and graphic designers.</p>
<p><a href="https://ageeky.com/wp-content/uploads/2015/08/aaexampleIMG_id1386238400_343178-300x197.jpg" target="_blank">Anti-aliasing emulates this in video games, taking a jagged line and doing some math to give the mind something to que on that will think it's seeing something smoother.</a> though it is a different means to the same goal(line detection and smart blurring, roughly speaking)</p>
<p>In fact, depending on how a game may be coded, rendering at 4k then downscaling to 1080 can be a viable alternative to AA.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/viperware" target="_blank">viperware</a>
			<div class="markdown"><blockquote>
<p>4K downscaled will always look better than 1080p because it is essentially averaging four pixels for ever pixel resulting in antialiasing and truer colors.</p>
</blockquote>
<p>This is the right answer. It is called supersampling.</p></div>		</li>
					</ul>
	