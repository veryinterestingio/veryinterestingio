	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Glaselar" target="_blank">Glaselar</a>
			<div class="markdown"><blockquote>
<p>...the surprisal of any two photographs (with the same size and colors depth)...</p>
</blockquote>
<p>This isn't the case in the scenario. If I understand you correctly, you're saying it's not possible to quantify entropy without a reference point? In that case, the image of the busy scene in the painting could be the baseline for comparison with the sky photo, right?</p>
<p>(Images in link in OP.)</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/energybased" target="_blank">energybased</a>
			<div class="markdown"><p>I have no idea what you mean by &quot;reference point&quot;.  <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" target="_blank">This</a> is the definition of <em>information theoretic entropy</em>.  <a href="https://en.wikipedia.org/wiki/Information_content" target="_blank">This</a> is the definition of <em>surprisal</em>.  You cannot have a surprisal without a <a href="https://en.wikipedia.org/wiki/Probability_space" target="_blank">probability space</a>, which includes a mapping from events to probabilities.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Glaselar" target="_blank">Glaselar</a>
			<div class="markdown"><p>I mean that a statistical model of the probable state of a system provides a reference against which to judge surprisal, and absent a probabilistic model for a painting, perhaps a static reference of the system (an image) in another state (depicting a different scene) could be the analog?</p>
<p>Thanks for the links - I'm reading.</p>
<blockquote>
<p>You cannot have a surprisal without a probability space](<a href="https://en.wikipedia.org/wiki/Probability_space" target="_blank">https://en.wikipedia.org/wiki/Probability_space</a>), which includes a mapping from events to probabilities.</p>
</blockquote>
<p>It seems like the author was maybe going with some vague hybrid interpretation of entropy that sits somewhere between the information theory definition based on probabilities and the thermodynamic definition based indirectly on order.</p></div>		</li>
					</ul>
		</ul>
		</ul>
	