	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/FractalJaguar" target="_blank">FractalJaguar</a>
			<div class="markdown"><p>Also there's an overhead involved in transferring each file. Copying one single 1GB file will be quicker than a thousand 1MB files.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/AY-VE-PEA" target="_blank">AY-VE-PEA</a>
			<div class="markdown"><p>Yes indeed, this is partially covered by &quot;fragmentation of data sectors&quot; as one thousand small files are going to be distributed a lot less chronologically than one file. I do not directly mention it though, thanks for adding.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/seriousnotshirley" target="_blank">seriousnotshirley</a>
			<div class="markdown"><p>The bigger effect is that for 1 million small files you have to do a million sets of filesystem operations. Finding out how big the file is, opening the file, closing the file. Along with that small file IO is going to be less efficient because file IO happens in blocks and the last block is usually not full. One large file will have one unfilled block, 1 million small files will have 1 million unfilled blocks.   </p>
<p>Further a large file may be just as fragmented over the disk. Individual files aren't guaranteed to be unfragmented.   </p>
<p>You can verify this by transferring from an SSD where seek times on files aren't an issue.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/littlefrank" target="_blank">littlefrank</a>
			<div class="markdown"><p>I worked for the IT department of a press agency that had an old server farm, when we got it updated to a larger, faster, cheaper private cloud I had to transfer 2.3PB of ~2MB files on the new farm. It took more than 3 months and we didn't transfer everything.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MyFellowMerkins" target="_blank">MyFellowMerkins</a>
			<div class="markdown"><p>A bigger reason is the associated overhead of opening and closing each of those files. Also, depending on exactly how many files, paging the inodes or file bitmap in and out of memory has a lot of overhead.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/JBinero" target="_blank">JBinero</a>
			<div class="markdown"><p>And this difference can be <em>substantial</em>, that is, not negligible. I once had to split a big deal 2GB file into roughly 150k small files. Copying speed went from 2 minutes to 20 hours.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NoRodent" target="_blank">NoRodent</a>
			<div class="markdown"><p>Yes, this is especially noticeable when you copy to or from a classic HDD. When copying between a flash drive and an SSD, the difference is much smaller.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/xxfay6" target="_blank">xxfay6</a>
			<div class="markdown"><p>Did the customer only have a floppy drive?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Shortbull" target="_blank">Shortbull</a>
			<div class="markdown"><p>So would archiving 1000 1MB files into one zip and then transferring it be better than transferring the 1GB outright as it is still one file but composed of many files, or is it irrelevant as they are still seperate?</p>
<p>I know zip files usually compress their contents and I understand some different types of compression algorithms but would banding all the 1000 1MB files be a form of it, or is it not possible? Would that not just make them into one &quot;chunk&quot; of data rather than many small fragments, one for each file?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Fresno_Bob_" target="_blank">Fresno_Bob_</a>
			<div class="markdown"><p>As far as the computer is concerned, the zip file is just one file.  It's no longer composed of many files, the container is purely abstract.  It's a single file generated by the data from many files with additional data for how the zip software can reconstitute the individual files.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mb271828" target="_blank">mb271828</a>
			<div class="markdown"><p>As far as the computer is concerned the zip file is just a single file so it should eliminate the overhead of having to transfer 1000 individual files, as well as the compression reducing the total amount of data to transfer.</p>
<p>I don't think the total time including archiving and extracting the other end would necessarily be less than just transferring the individual files though, [de]compression takes time and I think you'd incur a lot of the same overheads with dealing with lots of files when extracting the 1000 individual files at the other end. Though I suspect this would depend heavily on how and how far you were transferring the files, it's probably more efficient to compress and transfer an individual zip rather than 1000 files over the internet for example, but probably not when transferring from drive to drive locally.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Isogash" target="_blank">Isogash</a>
			<div class="markdown"><p>Probably, yes, but I won't claim to have enough knowledge to be absolutely certain without experimenting myself.</p>
<p>An archive format is a file format that contains a directory structure internally; the host OS treats it like a single file and you will get all of the behaviour of that (data probably not fragmented since it is treated as a unit, only one filesystem entry etc.). You can archive without compressing (if you've seen .tar.gz, the .tar is archive, and the .gz is for gzip compression), ZIP supports both.</p>
<p>If your transferring is significantly slowed due to the files being separate, then yes, transferring an archive would solve that problem. Transferring and storing a compressed archive is even better, since it's just a smaller file. However, creating the archive requires some overhead. You need to load the files into memory and then write them into the new file in the specified format. Calculating the point at which it's better to archive first is going to require way too many variables for me to guess for you unfortunately.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/myredditlogintoo" target="_blank">myredditlogintoo</a>
			<div class="markdown"><p>You're correct, but buffering is everywhere - the HDD has a buffer. Memory and cache memory are buffers. DMA transfers can use buffered queues. Destination device uses buffers, since flash memory must be erased and written a block at a time (there's actually a lot more to this, since wear leveling algorithms are used). Then there's the bus congestion and competition for processing resources.  So, you have lots and lots of places where slowdowns can occur. It's all rather bursty, and what you see is just a very high level approximation shown to the user.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/apathycoalition" target="_blank">apathycoalition</a>
			<div class="markdown"><p>It's not uncommon on Linux systems when doing file operations that something like a file copy is done almost entirely to filesystem cache (RAM), and then it reports the copy as done while leisurely writing it out to disk.  This is why it's important to safely remove USB flash drives and the like.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ztherion" target="_blank">ztherion</a>
			<div class="markdown"><p>On Linux you can force those buffers and caches to be flushed by running the <code>sync</code> command before unmounting any drives.</p>
<p>Anyone who's had to run etcd in production knows the pain of <code>fsync</code> and how much filesystems will lie to the user in the name of performance...</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mitakeet" target="_blank">mitakeet</a>
			<div class="markdown"><p>To build on this, some software reports the average of the transfer speed rather than instantaneous, so if yours steadily goes down, this is why.</p>
<p>If it fluctuates up at any point, then there was something else that was occupying space on the bus, such as another file transfer.</p></div>		</li>
					</ul>
	