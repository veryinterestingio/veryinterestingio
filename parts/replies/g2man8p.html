	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/PapoochCZ" target="_blank">PapoochCZ</a>
			<div class="markdown"><p>Upsampling is the same as image upscaling and the basic method can't add new information. </p>
<p>Nvidia shield uses neural networks to do the &quot;upscaling&quot;, so new information IS being added. It is inferred by the AI from the low quality input based on millions of training samples of other videos. </p>
<p>We could totally do the same with sound and I'm sure someone has already done something similar. It's not that we can't do it, it's that it's not very mainstream and nobody talks about it.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/pseudopad" target="_blank">pseudopad</a>
			<div class="markdown"><p>There is however no guarantee that the filled-in information is correct. It might look prettier, but not necessarily more true to reality.</p>
<p>The problem with sound is that if your sample rate is too low, high frequencies will not be recorded at all. If you have a 22 kHz recording, you'll have no frequencies above 11 kHz in the recording, because you need to sample a frequency twice before it's &quot;over&quot; in order to capture it accurately enough to make sure it was actually there.</p>
<p>That's also why CD quality is 44.1 kHz. That's enough to accurately capture up to 22.05 kHz, which is more than 99.9% of humans can hear. It's pretty much an audio format engineered to be as good as human hearing is.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Financial-Ad-7328" target="_blank">Financial-Ad-7328</a>
			<div class="markdown"><p>It is already full HD graphics so there is a lot of data to make close guess what is in the missing pixel. It wouldn't be used for scientific observations in this state but for other purposes it can work just fine.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NuftiMcDuffin" target="_blank">NuftiMcDuffin</a>
			<div class="markdown"><blockquote>
<p>If you have a 22 kHz recording, you'll have no frequencies above 11 kHz in the recording, because you need to sample a frequency twice before it's &quot;over&quot; in order to capture it accurately enough to make sure it was actually there.</p>
</blockquote>
<p>The last octave of audible sound really isn't that important though. Most adult's hearing doesn't go up to 20 kHz anyway, and even in the best case our ears aren't very sensitive to those frequencies. If you have a speaker that just cuts off somewhere above 10 kHz, chances are good that nobody will ever notice without playing an audio track specifically designed to point out that flaw. </p>
<p>That said, those frequencies can be recovered by a trained model. For example, if it's trained to recognize the spectrum of an instrument playing a particular note, it can fill in the blank based on the lower part of this spectrum. Of course, such a model might fill in those frequencies even if they don't belong there - which is an inherent problem with this kind of AI training.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/narrill" target="_blank">narrill</a>
			<div class="markdown"><p>OP is talking about bit rate, not sample rate</p></div>		</li>
					</ul>
		</ul>
		</ul>
	