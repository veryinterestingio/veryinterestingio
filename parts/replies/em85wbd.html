	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ensalys" target="_blank">ensalys</a>
			<div class="markdown"><p>So to convert this to a puzzle, if I grab 2 pieces who look like they obviously connect, the entropy is lower, than if I grab 2 pieces where I really have to look good to determine if they connect?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/VoilaVoilaWashington" target="_blank">VoilaVoilaWashington</a>
			<div class="markdown"><p>As long as the puzzle pieces are all shaped identically. </p>
<p>Obviously, a normal puzzle cut would be harder if it were all blue because there are no context clues but you still need to find the right spot.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/tehspoke" target="_blank">tehspoke</a>
			<div class="markdown"><p>If any 2 pieces grabbed at random can easily be seen to belong together or not then it is low entropy.</p>
<p>Essentially, most puzzles you buy are low (in fact 0) entropy - there is only one way it can fit together.  If I tell you where one piece in the complete puzzle sits (on the earth, with orientation provided) I know exactly where every other piece must be.  There is only one possible completed configuration.  Any local measurement of the system results in full information.</p>
<p>Assembling random (infinite) checker boards with white and black (nonalternating) squares, however, would have high entropy.  Just noting that a particular square was white tells you nothing about its neighbors.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Glaselar" target="_blank">Glaselar</a>
			<div class="markdown"><blockquote>
<p>Note that a pure blue (single tone) picture made in say, MS paint with auto fill has very low entropy, because any two small pieces that are the same shape <em>will be the same information</em> and hence you will readily identify them as the same</p>
</blockquote>
<p>Hmmm... If the universe is tending towards high entropy as everything averages out, doesn't that contradict this? I went to a series of 3 public Sean Carroll lectures where he used the example of coffee and cream mixing to a homogeneous blend as a microcosm of this. It seems like sampling the thoroughly-mixed, high-entropy coffee would be the same as your mono-blue sampling example, but you're saying that actually represents <em>low</em> entropy?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/walnut_Y_soybean" target="_blank">walnut_Y_soybean</a>
			<div class="markdown"><p>No, because mixing two parts together (high entropy) is not the same as having one single part (low entropy).</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Glaselar" target="_blank">Glaselar</a>
			<div class="markdown"><p>I realise you're not OP, but taking that and running with it...</p>
<ul>
<li>
<p>If you're talking about single-wavelength blue, ok I'm with you</p>
</li>
<li>If we're taking the MS Paint example literally, the image will be an RGB mix with 3 components. The explanation of entropy that's served me until today is that although there may be micro-pockets of ordering - the molecules in a milky coffee may lump together - on a macro scale our brains log it as a smooth mixture, and therefore we characterise that as high entropy.</li>
</ul>
<p>How do we therefore define a system where there's macro homogeneity (the same blue across a screen) and therefore <em>high</em> entropy, but micro ordering (pixels in a repeating array) so <em>low</em> entropy? Will it be the same as the coffee - high entropy? It's very ordered, but sampling still doesn't give you information on where you are on the screen.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MiffedMouse" target="_blank">MiffedMouse</a>
			<div class="markdown"><p>This answer misses an important issue with this example: the format of the picture matters a lot.</p>
<p>If you see any digital photo, there is an extremely high chance that this photo has been compressed. That compression algorithm will almost certainly reduce the range of blues such that the total entropy is very small.</p>
<p>In practical terms, almost all digital images will have an entropy that actually tracks fairly close to an intuitive understanding of information content (that is, busier = more entropy). That is precisely because the algorithms for storing image data are designed around human vision, and there is no reason to store data that the human brain doesn’t use. Thus, for a good compression algorithm, the perceived information should always be close to the actual information.</p>
<p>As an example, in the stack exchange discussion on this point some example images were shown. The “clear blue sky” image had less entropy than the old painting. <a href="https://dsp.stackexchange.com/questions/41592/does-a-simple-photograph-contain-more-information-than-a-complex-painting" target="_blank">link</a></p></div>		</li>
					</ul>
	