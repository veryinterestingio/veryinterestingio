	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/evenhub" target="_blank">evenhub</a>
			<div class="markdown"><p>That is absolutely incorrect. There are no “paths” in neural networks. In most architectures, all nodes of one layer connect to all nodes in the next layer. Some connections are a little stronger than others, sure, but conceiving it as a set of paths is extremely misleading—ALL weights ultimately come to form the final output of the neural net. </p>
<p>Additionally, your analogy of people “trampling soil” is a far cry from what actually happens in stochastic gradient descent. If you wanted to explain neural network training to a 5 year old, then why not just use the same hill climbing analogy that literally everyone else uses and doesn’t get wrong? </p>
<p>Source: years of deep learning research</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CountOmar" target="_blank">CountOmar</a>
			<div class="markdown"><p>Go ahead and use the hill climbing analogy then.  I'm all ears.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/eric_he" target="_blank">eric_he</a>
			<div class="markdown"><p>You’re right but you could be a little less rude about it</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/herptydurr" target="_blank">herptydurr</a>
			<div class="markdown"><p>That awkward moment when the top reply to all the top comments is &quot;nope, you're wrong&quot;</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/bradab" target="_blank">bradab</a>
			<div class="markdown"><p>This makes a lot of sense in the world of structural health monitoring where I first heard the term. Nodes at sensors “feeling” the most significant forces in the structure. Waiting for changes in the signals to evaluate the condition.</p></div>		</li>
					</ul>
	