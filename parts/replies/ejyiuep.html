	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Autism_researchers" target="_blank">Autism_researchers</a>
			<div class="markdown"><p>We appreciate the question. You are correct that running independent tests of separate hypotheses warrants correction for multiple comparisons. In our studies, as some of the first in the field, we often start with one hypothesis (i.e., association between prenatal vitamins and autism recurrence in high-risk siblings) and then expand analyses to include sub-analyses to address questions raised by the findings (i.e., does the association differ across subgroups (sex, race)? By timing? Across outcome subgroups?) or in response to reviewer comments (which nutrients are driving the association? Is there a dose-response). These additional analyses help us to understand the original question, rather than being separate hypotheses. Correcting for multiple comparisons would not change the findings, but would influence the significance of the findings. - RJS</p>
<p>I would add that it is also important to consider the consequences of ‘misses’; i.e., declaring a finding is not significant when it is in fact true (Type II error). Correcting for multiple comparisons makes this more likely and is especially problematic in many environmental epidemiology studies, which have deep exposure assessments but small sample size. One of the best ways to assess reliability is to see whether a finding can be replicated in additional independent studies. - CPL</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/DijonPepperberry" target="_blank">DijonPepperberry</a>
			<div class="markdown"><p>Thank you for the comprehensive answer. However, I find it still problematic.  The point of &quot;first survey&quot; style analyses is to generate hypotheses, not test them.  Many of the articles using your two cohort groups are specifically communicated <em>by the authors</em> as evidence of association, when really it is preliminary evidence of a potential hypothesis to be tested.</p>
<p>You have many primary studies based off of CHARGES, for example. Even if you only did one test per study, its important to correct for multiple comparisons based on the number of studies.</p>
<p>Obviously to generate hypotheses I think screening with .95s is ok (but, by definition, p hacking).  I think your group should consider the presentation of the implications of your research. It wouldn't change its importance.</p>
<p>I'm glad we agree that independent replication  is necessary. Unfortunately, due to the number of comparisons you've done, the chance of replicability (ie, an actual truth rather than random chance) is low.</p>
<p>And I find it disingenuous to say that correcting for multiple comparisons wouldn't change the results... it wouldn't change the numbers but most certainly your interpretations.  based off of the statements and conclusions in many of your papers, had CIs overlapped or p&lt;significance, you likely would not be stating it as evidence for an association.</p></div>		</li>
					</ul>
		</ul>
	