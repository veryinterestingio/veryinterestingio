	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Angel_Hunter_D" target="_blank">Angel_Hunter_D</a>
			<div class="markdown"><p>So now I have to wonder, why aren't negative results published as much? Sounds like a good way to save other researchers some effort.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/tuftonia" target="_blank">tuftonia</a>
			<div class="markdown"><p>Most experiments don’t work; if we published everything negative, the literature would be flooded with negative results. </p>
<p>That’s the explanation old timers will give, but in the age of digital publication, that makes far less sense. In a small sense, there’s a desire (subconscious or not) to not save your direct competitors some effort (thanks to publish or perish). There are a lot of problems with publication, peer review, and the tenure process…</p>
<p>I would still get behind publishing negative results</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/slimejumper" target="_blank">slimejumper</a>
			<div class="markdown"><p>negative results are not the same as experiments that don’t work.   confusing the two is why there is a lack of negative data in scientific literature.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Angel_Hunter_D" target="_blank">Angel_Hunter_D</a>
			<div class="markdown"><p>In the digital age it makes very little sense, with all the P-hacking we are flooded with useless data. We're even flooded with useful data, it's a real chore to go through. We need a better database system first, then publishing negative results (or even groups of negative results) would make more sense.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Kevin_Uxbridge" target="_blank">Kevin_Uxbridge</a>
			<div class="markdown"><p>Negative results <em>do</em> get published but you have to pitch them right.  You have to set up the problem as 'people expect these two groups to be very different but the tests show they're exactly the same!'  This isn't necessarily a bad result although it's sometimes a bit of a wank. It kinda begs the question of why you expected these two things to be different in the first place, and your answer should be better than 'some people thought so'.  Okay why did <em>they</em> expect them to be different?  Was it a good reason in the first place? </p>
<p>Bringing this back to p-hacking, one of the more subtle (and pernicious) ones is the 'fake bull-eye'.  Somebody gets a large dataset, it doesn't show anything like the effect they were hoping for, so they start combing through for something that does show a significant p-value.  People were, say, looking to see if the parent's marital status has some effect on political views, they find nothing, then combing about yields a significant p-value between mother's brother's age and political views (totally making this up, but you get the idea).  So they draw a bulls-eye around this by saying 'this is what we should have expected all along', and write a paper on how mother's brother's age predicts political views.</p>
<p>The pernicious thing is that this is an 'actual result' in that nobody cooked the books to get this result.  The problem is that it's likely just a statistical coincidence but you've got to publish <em>something</em> from all this so you try to fake up the reasoning on why you anticipated this result all along.  Sometimes people are honest enough to admit this result was 'unanticipated' but they often include back-thinking on 'why this makes sense' that can be hard to follow.  Once you've reviewed a few of these fake bulls-eyes you can get pretty good at spotting them. </p>
<p>This is one way p-hacking can lead to clutter that someone else has to clear up, and it's not easy to do so.  And don't get me wrong, I'm all for picking through your own data and finding weird things, but unless you can find a way to bulwark the reasoning behind an unanticipated result and test some new hypothesis that this result led you to, you should probably leave it in the drawer.  Follow it up, sure, but the onus should be on you to show this is a real thing, not just a random 'significant p-value'.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Cognitive_Dissonant" target="_blank">Cognitive_Dissonant</a>
			<div class="markdown"><p>Somebody already responded essentially this but I think it could maybe do with a rephrasing: a &quot;negative&quot; result as people refer to it here just means a result did not meet the p&lt;.05 statistical significance barrier. It is <em>not</em> evidence that the research hypothesis is false. It's not evidence of anything, other than your sample size was insufficient to detect the effect <em>if the effect even exists</em>. A &quot;negative&quot; result in this sense only concludes ignorance. A paper that concludes with no information is not one of interest to many readers (though the aggregate of no-conclusion papers hidden away about a particular effect or hypothesis is of <em>great</em> interest, it's a bit of a catch-22 unfortunately). </p>
<p>To get evidence of an actual negative result, i.e. evidence that the research hypothesis is false, you at least need to conduct some additional analysis (i.e., a power analysis) but this requires additional assumptions about the effect itself that are not always uncontroversial, and unfortunately the way science is done today in at least some fields sample sizes are way too small to reach sufficient power anyway.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Tidorith" target="_blank">Tidorith</a>
			<div class="markdown"><blockquote>
<p>it here just means a result did not meet the p&lt;.05 statistical significance barrier. It is not evidence that the research hypothesis is false.</p>
</blockquote>
<p>It is evidence of that though. Imagine you had 20 studies of the same sample size, possibly different methodologies. One cleared the p&lt;.05 statistical significance barrier, the other 19 did not. If we had just the one &quot;successful&quot; study, we would believe that there's likely an effect. But the presence of the other 19 studies indicates that it was likely a false positive result from the &quot;successful&quot; study.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/nguyenquyhy" target="_blank">nguyenquyhy</a>
			<div class="markdown"><p>That doesn't work either. You still need low p-value to conclude we have negative result. High p-value simply means your data is not statistical significant and that can come from a huge range of factors including error in performing the experiment. Contributing this kind of unreliable data make it very hard to trust any futher study on top. Regardless we need some objective way to gauge the reliability of a study, especially in a multidisciplinary environment nowadays. Unfortunately that means people will just game the system on whatever measurement we come up with.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/DuskyDay" target="_blank">DuskyDay</a>
			<div class="markdown"><p>The p-value is the probability of obtaining the data we see or more extreme <em>given</em> the null hypothesis is true.</p>
<p>A high p-value tells you the same thing as a low p-value, just with a different number for that probability.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/frisbeescientist" target="_blank">frisbeescientist</a>
			<div class="markdown"><p>I'm not sure I agree with that characterization. A high p-value can be pretty conclusive that X hypothesis isn't true. For example if you expect drug A to have a significant effect on mouse weight, and your data shows that mice with drug A are the same weight as those given a control, you've shown that drug A doesn't affect mouse weight. Now obviously there's many caveats including how much variability there was within cohorts, experimental design, power, etc, but just saying that you need a low p-value to prove a negative result seems incorrect to me. </p>
<p>And that kind of data can honestly be pretty interesting if only to save other researchers time, it's just not sexy and won't publish well. A few years ago I got some pretty definitive negative results showing a certain treatment didn't change a phenotype in fruit flies. We just dropped the project rather than do the full range of experiments necessary to publish an uninteresting paper in a low ranked journal.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Pyrrolic_Victory" target="_blank">Pyrrolic_Victory</a>
			<div class="markdown"><p>This gives rise to an interesting ethical debate</p>
<p>Suppose we are doing animal experiments on an anti inflammatory drug. Is it more ethical to keep doing new animal experiments to test different inflammatory scenarios and markers? Or is it more ethical to test as many markets as possible to minimise animal suffering and report results?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/WeTheAwesome" target="_blank">WeTheAwesome</a>
			<div class="markdown"><p>In vitro experiments first. There should be some justification for why you are running experiment on animals. Some external experiment or data that suggests you may see an effect if you run that experiment on  the animal. The hypothesis then should be  stated ahead of time before you do the experiment on the animal so there is no p-hacking by searching for lots of variables. </p>
<p>Now sometimes if the experiment is really costly, or limited due to ethics (e.g. animal experiments) you can look for multiple responses once but you have to run multiple hypothesis corrections on all the p values you calculate. You then need to run an independent experiment to verify that your finding is real.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/The-NullHypothesis" target="_blank">The-NullHypothesis</a>
			<div class="markdown"><p>Wouldn’t it depend on the animal?</p>
<p>I feel like no one is going to decry fungi, or insects being experimented on?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Greyswandir" target="_blank">Greyswandir</a>
			<div class="markdown"><p>Fungi are not animals</p>
<p>Depending on the purpose of the experiment there may be very little value to experimenting on non-mammalian animals.   The biology is just too different.   </p>
<p>But regarding the broader question, there are some circumstances where lab animals can be used for more than one experimental purpose (assuming the ethics board approves).   For example, my lab obtained rat carcasses from a lab that did biochemistry experiments.   Our lab had projects involving in vivo microscopy, so we didn’t care if the previous experiments had (potentially) messed up the animals chemistry, we just needed the anatomy to be intact.  </p>
<p>I never personally worked with animals, but most of the other people in my lab did.   At least the scientists I’ve known are very aware that their research is coming at the cost of animal’s lives and suffering, and they work to reduce or eliminate that when possible.   The flip side of that coin is that there just aren’t good ways of testing some things without using an animal</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/IWanTPunCake" target="_blank">IWanTPunCake</a>
			<div class="markdown"><p>fungi and insects are definitely not equal though. Unless I am misunderstanding your post.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Tin-Whiskers" target="_blank">Tin-Whiskers</a>
			<div class="markdown"><p>Good point yes. I've read a proposal to partially address the &quot;publish or perish&quot; nature of academia. Publications agree to publish a particular study before the study is concluded. They make the decision based on the hypothesis and agrees to publish the results regardless whether the outcome is positive or negative. This should in theory at least alleviate some pressure from researchers to resort to P hacking to begin with.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/arand0md00d" target="_blank">arand0md00d</a>
			<div class="markdown"><p>It's not solely the act of publishing, it's where you are being published. I could publish 30 papers a day in some garbage tier journal and my career will still go nowhere. To be a strong candidate for top jobs, scientists need to be publishing in top journals with high impact factors. If these top journals do this or at least make an offshoot journal for these types of studies then things might change.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/The-NullHypothesis" target="_blank">The-NullHypothesis</a>
			<div class="markdown"><p>Shouldn’t the top journals be the ones that best represent the science and have the best peers to peer review?</p>
<p>I think we skipped a step - why are the journals themselves being considered higher tier because they require scientists to keep publishing data?</p></div>		</li>
					</ul>
		</ul>
		</ul>
	