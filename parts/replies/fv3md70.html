	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MrHadrick" target="_blank">MrHadrick</a>
			<div class="markdown"><p>Is this the same rationale behind the 120gb update to warzone? They only have to optimise size depending on what's available</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/half3clipse" target="_blank">half3clipse</a>
			<div class="markdown"><p>Time-space trade off. </p>
<p>If you want to compress those hi res graphical assets, you can reduce the size, but that means the program needs to decompress them every time they use it, which takes processor time. games that aren't AAA level can get around this by just preloading everything, or at least a lot of everything into memory (if you've ever had a game that just sits there thinking for a while when it loads, probably doing that). Doesn't work so good when you'd need to preload 20 gb and the player may only have 6gb of memeory period. Even if you're clever about how you partially load stuff into memory, that also creates problems with pop in or load times, which players haaaate. Storing stuff uncompressed helps address that, since now there's a lot less overhead </p>
<p>Another aspect of the trade off of processing power vs storage space, is that storage space is really cheap these days and easily swapable, while increases to processing power are expensive and non trival to upgrade (or impossible in the case of consoles). You can buy a ssd large enough to hold a 120 gig AAA game for about the same cost as the game itself.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/GeenMachine" target="_blank">GeenMachine</a>
			<div class="markdown"><p>Most likely, it's tons of high-resolution textures and audio that is uncompressed. By not being compressed it loads much faster at the expense of your storage but streams into the engine more smoothly.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/_kellythomas_" target="_blank">_kellythomas_</a>
			<div class="markdown"><p>I was building a page that did some data processing earlier this week.</p>
<p>It loads a small 3 MB dataset and uses that to derive a larger dataset.</p>
<p>The simplest implementation just ran it as a single batch, but when complete the derived data consumes 1.5 GB of ram.</p>
<p>I was able to delay producing the derived data until the user had zoomed in to their area of interest and now a typical user might use between 200 and 300 MB of ram. (It depends how much they pan around, the pathological case is still 1.5 GB).</p>
<p>If there is time after all the more important features are complete I will implement culling so everything is cleaned up as it leaves the field of view. Then it will probably have an upper limit of 200 MB but that will only happen <em>if</em> I have time.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/catcatdoggy" target="_blank">catcatdoggy</a>
			<div class="markdown"><p>remember when getting jpgs and gifs down in size was part of my job. now everything is a PNG because who has time for that.</p></div>		</li>
					</ul>
	