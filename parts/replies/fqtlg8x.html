	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/chaossabre" target="_blank">chaossabre</a>
			<div class="markdown"><p>I would like to add that larger CPUs are also more costly to manufacture because the likelihood of defects increases with chip area. The bigger the chip, the fewer of them can be made from a singe silicon wafer, which means any defect in the wafer ruins a larger percentage of the chips made from it (lower yield), which drives up costs because a single defect wastes a larger area of silicon.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/porcelainvacation" target="_blank">porcelainvacation</a>
			<div class="markdown"><p>Modern large digital chips often have defect tolerance designed in, they put redundant core logic pieces around the layout that can basically be abandoned if they don't work correctly.</p>
<p>One of the biggest problems that wasn't mentioned, though, is reliability. Big chips expand and contract a lot with temperature cycling, and eventually vias and other high stress concentrated areas crack and fail. It is very difficult to get all of the metals, ceramics, semiconductors, and plastics in a chip to expand and contract at the same rate, so chips are often size limited by material properties and Mean Time Between Failure.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Amphorax" target="_blank">Amphorax</a>
			<div class="markdown"><p>Yep! Many chip manufacturers actually have only a few designs for their CPU, which are manufactured and tested. Only the most perfect dies are badged as the top-of-the-line processors, with all cores clocked as high as they will go. Defective dies are sorted based on the number of defects, and their defective cores are disabled. These defective chips are then sold as mid-range and bottom-end processors, with fewer active cores.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CartmansEvilTwin" target="_blank">CartmansEvilTwin</a>
			<div class="markdown"><p>AMD went down another road with their chiplet design. Instead of using a single chip with all components and low production yields they put several components on separate chips. This way they can even reuse older production processes for components that are not that crucial.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/potato1sgood" target="_blank">potato1sgood</a>
			<div class="markdown"><blockquote>
<p>Modern large digital chips often have defect tolerance designed in, they put redundant core logic pieces around the layout that can basically be abandoned if they don't work correctly.</p>
</blockquote>
<p>How complicated is it to design this?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RiPont" target="_blank">RiPont</a>
			<div class="markdown"><p>And to pre-empt the obvious question, &quot;why can't they just make bigger wafers of silicon?&quot;</p>
<p>Silicon wafers still suffer the laws of physics.  They start life as big cylinders, which are then cut into wafers.  They can't make the cylinders ever-bigger, because those cylinders start out as hot, due to the need to purify silicon into a homogeneous state, and have to cool.  The bigger the diameter of the cylinder, the more the heating/cooling makes the center of the cylinder (and therefore the wafer) have different physical properties than the outside.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/I__Know__Stuff" target="_blank">I__Know__Stuff</a>
			<div class="markdown"><p>They do keep making bigger wafers. In the 80s, they went from 4” wafers to 6”, then to 8” in the 90s, and 12” (about 30 cm) in about 2002. <del>Now they’re using 45 cm wafers.</del></p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/nolo_me" target="_blank">nolo_me</a>
			<div class="markdown"><p>Most larger CPUs these days (Threadripper, Epyc etc) are made by assembling multiple smaller dies into a larger package.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/fichtenmoped" target="_blank">fichtenmoped</a>
			<div class="markdown"><p>Another point is if you just use more cores you have to optimize the software for parallel computing, which makes software development more complicated.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/PlayMp1" target="_blank">PlayMp1</a>
			<div class="markdown"><p>However, as clock speed gains have basically halted (5GHz, maybe up to 5.2GHz with a golden chip, seems to be the highest anyone can go on any kind of practical cooling solution - i.e., not bathing the CPU in liquid nitrogen), it's become trickier and trickier to find more improvements within instructions per clock and such, and going for more and more cores has become the norm. If you look at the highest end consumer desktop CPUs from Intel and AMD, AMD (who has historically focused on more cores over higher clocks) has the 16-core, 32 thread Ryzen 3950X, and Intel (who has historically focused on higher clocks over more cores) has the i9-9900k at 8 cores and 16 threads.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/suicidaleggroll" target="_blank">suicidaleggroll</a>
			<div class="markdown"><p>It’s not just about cooling, it’s about the practicality of faster clock speeds, physical distances become pretty limiting at those speeds.  The speed of light in a vacuum is 3e8 m/s.  The speed of light through copper is usually around 70% of that, so 2e8 m/s.  At 5 GHz, that means your signal only has time to travel 40mm before the next clock cycle.  It doesn’t matter how fast your CPU is running if it’s talking to cache that’s 1cm away, or memory that’s 10cm away, all you’re doing is spending more cycles twiddling your thumbs before the response comes back.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Defoler" target="_blank">Defoler</a>
			<div class="markdown"><p>Intel new series can go to 5.3ghz on basic boost. So under OC you might expect 5.5ghz.</p>
<p>But intel are stuck at 14nm which hinders how much more they can push the speed.
AMD tech still can't reach those speeds.</p>
<p>If intel moves to 7nm and they can push speeds to new limits, you might see in the future 6ghz core speeds if intel keeps concentrating on speed over higher core count.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RedSpikeyThing" target="_blank">RedSpikeyThing</a>
			<div class="markdown"><p>This itself is a whole other can of worms because for the vast majority of applications performance doesn't much matter (within reason). As a result developers have little incentive to make their app any more efficient than it needs to be because doing so takes time (i.e. costs money) that they could have spent building something else. This a classic case of high opportunity cost.</p>
<p>In some sense this is good because consumers like shiny new things and developers can focus on building those shiny new things, rather than tweaking performance for things that really are good enough. In another sense it's bad because if every application was more efficient then you could do more with the same hardware.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/LonelySnowSheep" target="_blank">LonelySnowSheep</a>
			<div class="markdown"><p>Well I would argue that a majority of applications that require these high-end cpus are actually programmed for performance</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/manycactus" target="_blank">manycactus</a>
			<div class="markdown"><blockquote>
<p>and there's a point when 90% of users don't need anything that powerful </p>
</blockquote>
<p>Just give the software folks a few more mo[n]ths to bloat the fuck out of their shitty apps.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/hanswurst_throwaway" target="_blank">hanswurst_throwaway</a>
			<div class="markdown"><p>You want to see a website <em>and</em> listen to spotifiy <em>and</em> take notes on a word-document at the same time? better upgrade to 32GB RAM</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/DdCno1" target="_blank">DdCno1</a>
			<div class="markdown"><p>I upgraded from 8 to 24GB a few years ago. I did this mainly for 3D renders, but I immediately noticed by how much this increased the usability of the PC at a time when eight gigs were still considered sufficient for most users.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/jambox888" target="_blank">jambox888</a>
			<div class="markdown"><p>The problem is more that people want 100 chrome tabs open at once. Antivirus is another huge memory hog.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/thatLifeVibe" target="_blank">thatLifeVibe</a>
			<div class="markdown"><p>Electron is a terrible framework that never should've existed. Actually node.js in general</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/thegoldengamer123" target="_blank">thegoldengamer123</a>
			<div class="markdown"><p>Well developer time is expensive compared to ram prices, so abstractions that take more ram but are easier for developers are very popular.</p></div>		</li>
					</ul>
		</ul>
	