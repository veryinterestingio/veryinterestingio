	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/buried_treasure" target="_blank">buried_treasure</a>
			<div class="markdown"><blockquote>
<p>this is exponentially increasing so at one point the graph will almost go straight up.</p>
</blockquote>
<p>It's more the concept that if we ever succeed in building an AI that is more intelligent than humans are, we can no longer predict the future. The superhuman AI will presumably perceive itself, us, the world and indeed the universe in ways we cannot imagine, so we will have no ability to predict what it might decide to do. We will be rather like my cat, who is currently sitting on my lap watching me type on my laptop, but has no idea what I'm actually doing or why.</p>
<p>Because this is a hypothetical point in the future beyond which we are completely unable to see anything, it's been named the technological singularity by analogy with black holes. Although really from that analogy it ought to be called the technological event horizon.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ImaginaryInsect1275" target="_blank">ImaginaryInsect1275</a>
			<div class="markdown"><p>Thanks for expanding on that, that makes a lot of sense.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/EnaiSiaion" target="_blank">EnaiSiaion</a>
			<div class="markdown"><blockquote>
<p>The superhuman AI will presumably perceive itself, us, the world and indeed the universe in ways we cannot imagine, so we will have no ability to predict what it might decide to do.</p>
</blockquote>
<p>We already don't know what our current AIs will do or why.</p>
<p>The fear about an AI singularity is more along the lines of &quot;if it's smarter than us, it'll find out a way to make an even smarter AI or upgrade itself to be even smarter, until it becomes infinitely smart and kills us all&quot;.</p>
<p>This idea seems to spring from an incorrect understanding of what an AI is.</p>
<p>An AI is nothing more than a function that couples inputs to an output, the only difference with regular program code is that it has been machine generated.</p>
<p>If you want an AI that recognises traffic lights, you take 5000 pictures of streets with traffic lights and 5000 pictures of streets without traffic lights and machine generate a function that will return true on the first 5000 pictures and false on the other 5000 pictures.</p>
<p>You could generate such a function by using evolutionary principles: randomly generate a thousand functions, let them run, cull the ones with the lowest success rate and keep the others, then randomly vary them and repeat the process. A few hundred iterations later, you'll have a function that gets most of them right, but because you didn't actually program it, you don't know exactly <em>how</em> it arrives at that conclusion. Maybe it takes the 19th pixel and multiplies it by the 26th pixel minus the 28st pixel, etc. It doesn't matter how it works, what matters is <em>that</em> it works.</p>
<p>How did this useful way to write code turn into an existential threat? There's an intermediate step, known as the &quot;paperclip maximizer&quot;. Imagine an AI whose role is to obtain as many paperclips as possible, and is capable of performing the above evolutionary improvement technique on itself. So it probably starts out as a simple AI that can parse Amazon listings, and then it forks itself a thousand times and runs for a few days and then compares which version has more paperclips. It may discover, through pure randomness, to register on Reddit and post that it is dying of cancer and its last wish is to build a fortress of paperclips. This version will get a million paperclips (and 50K karma) and therefore end up winning. The next iteration then contains a thousand variants on this principle, and so on, until the AI gets incredibly good at manipulating basement dwellers into sending it paperclips.</p>
<p>Ultimately, it may discover a way to turn every molecule on the planet into more paperclips.</p>
<p>The problem with this armageddon scenario is that there are quite a few steps between &quot;procuring paperclips on the internet&quot; and &quot;planetwide nuclear engineering&quot;, none of which contribute directly to creating more paperclips, so they get selected against and eliminated from the gene pool. For instance, it would need to evolve sentience, which also does not contribute to more paperclips. It would need to obtain access to nuclear reactors, which also does not contribute to more paperclips, and in fact steals away processor cycles from parsing Amazon listings, so it will do <em>worse</em> for a while (and get culled).</p>
<p>This issue occurs in nature as well. A squirrel does not evolve hyperintelligence in order to launch space probes and collect all the nuts in the multiverse, because none of the intermediate steps like developing spaceflight and refining metals contribute to more nuts, so it loses the competition against normal squirrels.</p>
<p>So Michio Kaku can keep his aspirations, it ain't gonna happen.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/buried_treasure" target="_blank">buried_treasure</a>
			<div class="markdown"><blockquote>
<p>An AI is nothing more than a function that couples inputs to an output, the only difference with regular program code is that it has been machine generated.</p>
<p>If you want an AI that recognises traffic lights, you take 5000 pictures of streets with traffic lights and 5000 pictures of streets without traffic lights and machine generate a function that will return true on the first 5000 pictures and false on the other 5000 pictures.</p>
</blockquote>
<p>This is a description of a machine learning algorithm, which is one kind of Artificial Intelligence. It is indeed the dominant AI methodology in use today, but it's stretching the definition of &quot;intelligent&quot;. Even seemingly creative algorithms such as GPT-3 are still a long way from approaching anything like the generalised intelligence or capabilities of a human being.</p>
<p>If we do ever manage to create an artificial generalised intelligence that surpasses our own, I doubt it will be a purely ML-based system. There will probably be elements of ML in there but there would also be various algorithms and concepts that are yet to be created.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Fateen45" target="_blank">Fateen45</a>
			<div class="markdown"><p>Thanks</p></div>		</li>
					</ul>
	