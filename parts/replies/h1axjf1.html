	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/awkward_replies_2" target="_blank">awkward_replies_2</a>
			<div class="markdown"><p>If your core product is an end user application and not the applications installer I understand that. </p>
<p>But what if your core product IS an installer (e g. InstallShield?) or is constantly installed by millions of users every day (think windows update)...</p>
<p>Wouldn't it make sense to have installers use machine learning from every user's machine data (e.g. CPU speeds, OS version, disk type, and how they influence how long the installer took for each step, etc.) to enhance installer step duration predictions? For work OS and Software, an update taking three seconds, three minutes, thirty minutes or three hours makes a massive difference because the cost of having the user go through it on work time can be massive!</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/et27U4Y4qse0AIcyFZg8" target="_blank">et27U4Y4qse0AIcyFZg8</a>
			<div class="markdown"><p>No. That's a huge waste of engineering resources and ML infrastructure. That stuff isn't free. I would <em>never</em> be able to convince my boss to spend money on that, especially when it doesn't actually make anything faster. Users are still gonna have to sit through it.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/useablelobster2" target="_blank">useablelobster2</a>
			<div class="markdown"><blockquote>
<p>That stuff isn't free.</p>
</blockquote>
<p>Slight understatement there.</p>
<p>That stuff is absurdly expensive, to the point where startups using it for serious applications with real market potential still go bankrupt due to the costs involved. One mistake in your model can mean all that money and compute time is wasted, and even if everything worked it still might need multiple refinements, each of which take more time and money.</p>
<p>Accurate loading bars simply aren't worth it.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/torbeindallas" target="_blank">torbeindallas</a>
			<div class="markdown"><p>Yes, you could do that, but even then it won't be that much more accurate. </p>
<p>Eg. a HDD. Read/write speeds change with the location on the disk, starting with fastest and slowing down as we reach the end of the medium. A HDD may also be fragmented due to files coming and going and the disk being nearly full. It might have a defect or hardware error, that may cause it to stall for minutes in certain situations. (I've tried this one, only realized it when the drive died completely, and suddenly everything else was fast again).</p>
<p>SSDs have similar ways of being unpredictable in performance. Some are super fast for the first couple of seconds, then slow down as pages need to be cleared before you can write them.</p>
<p>Cheap CPUs have less cache than expensive ones, and a program may be written in a way that there are minimal cache misses on the expensive one, but the cheap one doesn't fit in the cache, and suddenly everything is much slower.</p>
<p>Other applications running can have a significant effect on the performance of your program - and this can change at any time during the installation process.</p></div>		</li>
					</ul>
		</ul>
	