	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/trumpeting_in_corrid" target="_blank">trumpeting_in_corrid</a>
			<div class="markdown"><p>I'm sorry if my question came across as being from a 'know-it-all' I really didn't mean it like that. I didn't really know how to formulate the question. Thank you for your answer.
A friend of mine insists that there was substance to that study (that is, that it's true that MMR vaccines can cause autism) and that the revelation that it was not valid came under pressure from the pharmaceutical industry. His trump card is that 'it was published in The Lancet' i.e. a journal that wouldn't have accepted a study without making sure it was conducted the way it should be. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/cantgetno197" target="_blank">cantgetno197</a>
			<div class="markdown"><blockquote>
<p>without making sure it was conducted the way it should be.</p>
</blockquote>
<p>None of science works like this. I think there's an idea that when you do some $100,000 study on &quot;blah&quot; and submit the results to a peer reviewed journal that those peer reviewers, like, fly out to your lab and examine all your equipment and use $300,000 of their own to replicate your results 3 times and pour over your 30 pages of raw data looking for inconsistencies and redo all your statistics themselves to make sure it adds up.</p>
<p>In reality, they spend an afternoon reading through your manuscript and taking it at face value.  Unless something immediately screams &quot;fishy&quot; about your stats they'll assume you did them as you said you did. If you say you used method X and X is well known to over-estimate Y and you say Y is high, THAT'S something they will catch you on and grill you.  But if you say you use method X and actually used method Y, it's often not possible for them to know unless your results are obviously not possible using method X. All they have is what you say and how that fits with existing literature to assess and all the time they're going to spend is probably an afternoon.</p>
<p>So a peer reviewer is an expert who spends an afternoon taking your paper at face value and establishing: is the work significant, are there any methodological shortcomings as describes, are the conclusions matched by the data.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/trumpeting_in_corrid" target="_blank">trumpeting_in_corrid</a>
			<div class="markdown"><p>Thank you. That is very helpful.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mfb-" target="_blank">mfb-</a>
			<div class="markdown"><blockquote>
<p>are the conclusions matched by the data.</p>
</blockquote>
<p>Or &quot;matched by what you show about the data&quot;. Peer review can't catch if you just claim more vaccinated children got autism than in reality.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/slicermd" target="_blank">slicermd</a>
			<div class="markdown"><p>Peer reviewers are not necessarily ‘experts’ in the sense that people would expect, either.  I know this is true, because I do some peer review, and have no such qualifier whatsoever.  Smaller journals sometimes have to take what they can get.  I do try to take it seriously, but have definitely seen some papers published that I recommended against.  Peer review doesn’t require unanimity and the editor has the final say ?‍♂️</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NeuroBill" target="_blank">NeuroBill</a>
			<div class="markdown"><p>Nono, it didn't come across as know it all.</p>
<p>The big mistake your friend is making (and that a lot of people make) is thinking that one scientific paper by itself means anything. I'm about to say something that may seem very strange, but read it carefully: The simple statistical fact is that Wakefield's publication could, theoretically, have been generated without any wrong doing whatsoever. I say this because all measurements have error, and all samples can be far away from true population. e.g. I could set up a test to see if aspirin causes cancer, and give it to 100 people, and just by dumb luck, all 100 could get cancer. It would be extremely unlikely, but it's possible: coincidences happen. This is why we repeat things. It would be incredibly unlikely that all 100 of my patients got cancer, but if <em>you</em> did the study again, with new patients, and all of them got cancer too... we the chance of that happening if aspirin didn't cause cancer would be astronomically low. importantly, having the study repeated by another group takes care of a lot of things: maybe I had financial interest in saying aspirin is bad. Or maybe my batch of aspirin was contaminated. etc etc etc... i.e. the repetition not only dealt with statistical problems, it dealt with other problems too.</p>
<p>That's why single papers, by themselves, are generally not of much use, especially when it comes to nasty biological things that are super messy and complex and variable. That's why policy makers, in general, don't care about the findings of one paper: the look at &quot;meta-analysis&quot; where the findings of lots of papers are combined, to give a better over all understanding.</p>
<p>And finally, the beauty of this case is that Wakefield's observation WAS repeated, in a very large study, of just under 100,000 children, in a study specifically designed to test for autism. And guess what they found? <a href="https://jamanetwork.com/journals/jama/fullarticle/2275444" target="_blank">No association, even in high risk groups</a>. And that is the real reason you should believe that MMR vaccines are safe: not because Wakefield was struck off, not because he had conflicts of interest, not because he misreported his study design, not because he is generally an unpleasant crook, but because science took his claims seriously, and showed that they were false in an overwhelming way. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/trumpeting_in_corrid" target="_blank">trumpeting_in_corrid</a>
			<div class="markdown"><p>Thank you for your detailed answer.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ephemeralista" target="_blank">ephemeralista</a>
			<div class="markdown"><p>+10 for meta-analyses. Much more reliable than one-off experiments.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mfukar" target="_blank">mfukar</a>
			<div class="markdown"><p>It was also retracted by the Lancet, so the statement &quot;<em>it was published in The Lancet</em> (, therefore it must be true / correct)&quot; is cherry-picking the facts that fit an opinion, and not a valid line of reasoning at all.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/trumpeting_in_corrid" target="_blank">trumpeting_in_corrid</a>
			<div class="markdown"><p>That's right. Thank you for pointing that out.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/bunnicula9000" target="_blank">bunnicula9000</a>
			<div class="markdown"><blockquote>
<p>The journals that already have a high impact factor (like the Lancet) instead like it when their articles are in the news. One might wonder if the editors of The Lancet also considered how much press coverage this article would generate.</p>
</blockquote>
<p>There's an internal debate going on at Nature currently about whether the flagship journal is accepting papers based on their likelihood of making the news rather than based on them being good science, or innovative, or etc. So yeah &quot;will this article generate press coverage&quot; is (a) definitely something journal editors care about a whole lot and (b) may or may not overshadow a paper's other merits or lack of merit.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/bnannedfrommelsc" target="_blank">bnannedfrommelsc</a>
			<div class="markdown"><p>Which is why funding can have a huge impact. If you say you're going to study how colgate makes your teeth whiter or you say you're going to study how a competitor makes your teeth whiter, who do you think is going to get funded?</p></div>		</li>
					</ul>
	