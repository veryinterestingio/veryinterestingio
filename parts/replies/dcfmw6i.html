	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/bfevans19" target="_blank">bfevans19</a>
			<div class="markdown"><p>Are there any books on this topic written at a pop-sci level? </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/PipFoweraker" target="_blank">PipFoweraker</a>
			<div class="markdown"><p><em>The Information</em>, by James Gleick, is a reasonable toe-in-the-pool experience, IIRC.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/AndreDaGiant" target="_blank">AndreDaGiant</a>
			<div class="markdown"><p>Though I loved it and found it a great pop-sci intro to Information Theory, I can't recall it mentioning arrow of time or physical representations of entropy.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/chakravanti" target="_blank">chakravanti</a>
			<div class="markdown"><p>He wrote <em>Choas</em> as well and I highly reccomend that.  On par with some of the most valuable psychedelic experiences I've had.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Naska25" target="_blank">Naska25</a>
			<div class="markdown"><p>Entropy demistified - Arieh Ben-Naim there are two version, one si full of theory the other one is easier ?</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ericGraves" target="_blank">ericGraves</a>
			<div class="markdown"><p>Not pop-sci, but information theory is fortunate enough to have one of the most accessible entry level texts ever written about it. Specifically, <em>Elements of Information Theory</em> by Thomas Cover and Joy Thomas. The book assumes a very basic understanding of probability (bayes rule, law of total probability, law of unconscious statistician) and then builds a number of deep theorems, <strong>with a plethora of examples</strong>. </p>
<p>It is amazingly well written, and I would thoroughly recommend it if you know just that little bit of probability theory. And, here are some of the topics covered:</p>
<ul>
<li>discrete entropy, mutual information, and KL-divergence,</li>
<li>data processing inequality (any function applied to data reduces the amount of information within),</li>
<li>Fano's inequality (entropy bound based on probability of reconstructing the original quantity),</li>
<li>optimal gambling,</li>
<li>lossless compression,</li>
<li>channel capacity and error correction coding,</li>
<li>entropy over continuous random variables,</li>
<li>rate-distortion theory (analog of lossy compression),</li>
<li>statistics (Sanovs theorem, Neyman-Pearson, Fisher information)</li>
<li>distributed source coding,</li>
<li>Kolmogorov complexity,</li>
<li>multi-terminal information theory,</li>
<li>maximum entropy (this is the connection to the stuff we are talking about here),</li>
<li>stock portfolio optimization.</li>
</ul>
<p>I mean, I can not recommend the book enough. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Lichewitz" target="_blank">Lichewitz</a>
			<div class="markdown"><blockquote>
<p>They can both be defined as functions of a probability distribution.</p>
</blockquote>
<p>But that's enough to say they're essentially the same? Anyways, thank you A LOT for your answer, I'll make sure to read the links you provided</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ericGraves" target="_blank">ericGraves</a>
			<div class="markdown"><p>That was suppose to be a comma at the end of my first sentence. And the second sentence was the important part. The formulas for Shannon entropy is</p>
<pre><code>-Σ p log p</code></pre>
<p>while for Gibbs it is</p>
<pre><code>-K Σ p log p</code></pre>
<p>where K is a constant. In thermodynamics problems, in equilibrium Gibbs becomes K log W. No such assumptions are typical in a general information theory problem.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/monarc" target="_blank">monarc</a>
			<div class="markdown"><p>A <a href="https://www.reddit.com/r/askscience/comments/5aji19/physics_is_entropy_quantifiable_and_if_so_what/" target="_blank">recent discussion here</a> might be of interest - lots of great answers.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Javlington" target="_blank">Javlington</a>
			<div class="markdown"><p>I also highly recommend the BBC documentary 'Order and Disorder' which deals with these exact topics and explores them.</p></div>		</li>
					</ul>
	